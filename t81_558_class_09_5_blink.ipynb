{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZggjUZ5oPvzH"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_deep_learning/blob/main/t81_558_class_09_5_blink.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDTXd8-Lmp8Q"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "\n",
    "**Module 9: Facial Recognition**\n",
    "\n",
    "- Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "- For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncNrAEpzmp8S"
   },
   "source": [
    "# Module 9 Material\n",
    "\n",
    "- Part 9.1: Detecting Faces in an Image [[Video]](https://www.youtube.com/watch?v=Hpp3D3P2iWQ&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_09_1_faces.ipynb)\n",
    "- Part 9.2: Detecting Facial Features [[Video]](https://www.youtube.com/watch?v=AblTbq0T2wE&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_09_2_face_features.ipynb)\n",
    "- Part 9.3: Reality Augmentation [[Video]](https://www.youtube.com/watch?v=jfZDiRxx5Bc&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_09_3_reality_augmentation.ipynb)\n",
    "- Part 9.4: Application: Emotion Detection [[Video]](https://www.youtube.com/watch?v=F0H6vojQhE8&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_09_4_emotion.ipynb)\n",
    "- **Part 9.5: Application: Blink Efficiency** [[Video]](https://www.youtube.com/watch?v=96LPEStHCUA&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_09_5_blink.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKQqQljyPvzK"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code checks that Google CoLab is and sets up the correct hardware settings for PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fU9UhAxTmp8S",
    "outputId": "86adae36-e622-4aff-caf0-a23a29e2c180"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# Make use of a GPU or MPS (Apple) if one is available.  (see module 3.2)\n",
    "import torch\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "if device!='cuda':\n",
    "  print(\"*******WARNING, this notebook requires a CUDA GPU****\")\n",
    "  print(\"This notebook will not work correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-qb-mcqmp8U"
   },
   "source": [
    "# Part 9.5: Blink Efficiency\n",
    "\n",
    "In the transformative realm of machine learning, the frontier of healthcare has emerged as a prime domain for the application of these innovative techniques. With the capacity to predict, classify, and even potentially prescribe, machine learning is revolutionizing medical diagnosis and treatment strategies. One such application is the precise mapping and recognition of facial landmarks. These anatomical points on the human face, not limited merely to identity verification or augmented reality applications, possess profound significance in the medical sector.\n",
    "\n",
    "Facial landmarks, a series of strategically identified points on a face, can be traced and tracked to gauge a range of physiological and anatomical anomalies. From understanding facial asymmetry to identifying features of genetic disorders, these landmarks offer a nuanced understanding of the facial anatomy, allowing for the diagnosis and monitoring of various medical conditions.\n",
    "\n",
    "One intriguing application is in the diagnosis of \"lagophthalmos\" â€“ a condition where an individual cannot close one or both eyes completely during a blink. Early detection is vital as the incomplete closure of the eye can lead to complications like dry eye and even corneal ulcerations. Traditional methods, often relying on subjective assessments or invasive procedures, sometimes miss the subtle onset of this condition. However, with machine learning's prowess and the use of video analysis, we can now obtain a dynamic understanding of the blink cycle.\n",
    "\n",
    "By analyzing videos of the blink cycle using facial landmark detection, it becomes feasible to accurately measure the extent of eyelid closure in real-time, identifying even minor discrepancies in the blink mechanism. Machine learning models can be trained to detect the specific landmarks associated with the upper and lower eyelids, evaluating their proximity and movement during a blink. This objective and quantifiable method ensures timely diagnosis and intervention, enhancing patient outcomes.\n",
    "\n",
    "In this section, we will delve deep into the technicalities of how machine learning aids in the detection of lagophthalmos using facial landmarks. We will explore the algorithms, methodologies, and challenges involved, and how this pioneering approach is making waves in the field of ophthalmology.\n",
    "\n",
    "We begin by installing SPIGA and Facenet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53sLzbmvxYSB",
    "outputId": "e087171c-8e98-47c9-efc2-18ea8f6cb669"
   },
   "outputs": [],
   "source": [
    "# Setup SPIGA and facenet\n",
    "!pip install facenet-pytorch\n",
    "!git clone https://github.com/andresprados/SPIGA.git\n",
    "%cd SPIGA/\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfXNE32xIsSp"
   },
   "source": [
    "We will measure blink efficency by looking at how far open each of the patients eyes are during a blink cycle. We will analyze each frame of the video clip and measure the area between the eyelids for both eyes over each frame. We load an image of a patient for analysis. Note, that this image was used with permission of the pateint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JNuIn5cxeH6"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "#url = \"https://s3.amazonaws.com/data.heatonresearch.com/images/jeff/about-jeff-heaton-2020.jpg\"\n",
    "url = \"https://data.heatonresearch.com/images/wustl/data/blink-frame.jpg\"\n",
    "\n",
    "# Download the image using requests\n",
    "response = requests.get(url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "response.raise_for_status()\n",
    "\n",
    "# Convert the downloaded bytes to a numpy array\n",
    "image = np.asarray(bytearray(response.content), dtype=\"uint8\")\n",
    "\n",
    "# Decode the numpy array to an OpenCV image\n",
    "img = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "\n",
    "# Convert the OpenCV image (NumPy array) to a PIL Image\n",
    "image_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "image_pil = Image.fromarray(image_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lmDP6e6MsQc"
   },
   "source": [
    "We are now ready to call MTCNN to find the face and SPIGA to obtain the facial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v3Dz9cg2xkHt",
    "outputId": "2aac3eec-a379-4da0-bc90-4a7fae52dd62"
   },
   "outputs": [],
   "source": [
    "from spiga.inference.config import ModelConfig\n",
    "from spiga.inference.framework import SPIGAFramework\n",
    "import copy\n",
    "\n",
    "mtcnn = MTCNN(keep_all=False, device=device)\n",
    "\n",
    "# Detect faces\n",
    "boxes, _ = mtcnn.detect(image_pil)\n",
    "\n",
    "# Create a bounding box for the face we just detected.\n",
    "bbox = [\n",
    "    boxes[0][0],\n",
    "    boxes[0][1],\n",
    "    boxes[0][2],\n",
    "    boxes[0][3]]\n",
    "\n",
    "# Process image\n",
    "dataset = 'wflw'\n",
    "processor = SPIGAFramework(ModelConfig(dataset))\n",
    "features = processor.inference(img, [bbox])\n",
    "\n",
    "# Prepare variables\n",
    "canvas = copy.deepcopy(img)\n",
    "landmarks = np.array(features['landmarks'][0])\n",
    "headpose = np.array(features['headpose'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kd53NJ5sPe7Z"
   },
   "source": [
    "# Analyzing and Measure Eye Area\n",
    "\n",
    "We must complete several steps to determine the difference in area between two eyes in an image. The first step is determining the number of pixels per millimeter in the computer image. This technique allows us to convert pixel areas on the image to actual millimeter lengths and areas. To do this, we will measure the number of pixels between the two pupils (landmarks 96 and 97) in the patient's eyes. We refer to the distance between the two pupils as the pupillary distance, which is 63 millimeters for most people. By dividing 63 by pupillary length, we can calculate how to convert pixels to millimeters; this is the **pix2mm** variable in the code below.\n",
    "\n",
    "Next, we will use each eye's landmark perimeter coordinates to calculate each eye's area. We calculate this area with the shoelace method. The [shoelace formula](https://stackoverflow.com/questions/24467972/calculate-area-of-polygon-given-x-y-coordinates), also known as Gauss's area formula or the surveyor's formula, is a method for finding the area of a polygon when given the coordinates of its vertices. The name \"shoelace\" comes from the \"crisscross\" manner in which the coordinates are used to compute the area, resembling the lacing of shoes.\n",
    "\n",
    "Next, we measure the polygon of each eye and overlay a blue tone to show where the eyes were detected. Once this is complete, we display the area of each eye and the difference between them. The bigger the difference, the more indication of a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "id": "TvlDJu5Rxoll",
    "outputId": "2b63d966-c783-4770-b8f4-1f02ae2f23a7"
   },
   "outputs": [],
   "source": [
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# determine scale of image\n",
    "pupillary_distance = abs(landmarks[96] - landmarks[97])[0]\n",
    "pix2mm = 63/pupillary_distance\n",
    "print(f\"1 pixel is equal to {pix2mm} mm\")\n",
    "\n",
    "# Shoelace, https://stackoverflow.com/questions/24467972/calculate-area-of-polygon-given-x-y-coordinates\n",
    "def PolyArea(x,y):\n",
    "    return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))\n",
    "\n",
    "\n",
    "def measure_polygon(canvas, contours, pix2mm, alpha=0.4, color=(255,0,0)):\n",
    "  contours = np.array(contours)\n",
    "  overlay = canvas.copy()\n",
    "  pts = np.int32(contours)\n",
    "  cv2.fillPoly(overlay, pts = [pts], color =color)\n",
    "  canvas = cv2.addWeighted(overlay, alpha, canvas, 1 - alpha, 0)\n",
    "  contours = contours*pix2mm\n",
    "  x = contours[:,0]\n",
    "  y = contours[:,1]\n",
    "  return PolyArea(x,y), canvas\n",
    "print(\"here\")\n",
    "\n",
    "# Show image results\n",
    "right_eye_area, canvas = measure_polygon(\n",
    "  canvas,\n",
    "  [landmarks[60],\n",
    "  landmarks[61],\n",
    "  landmarks[62],\n",
    "  landmarks[63],\n",
    "  landmarks[64],\n",
    "  landmarks[65],\n",
    "  landmarks[66],\n",
    "  landmarks[67]], pix2mm)\n",
    "\n",
    "left_eye_area, canvas = measure_polygon(\n",
    "  canvas,\n",
    "  [landmarks[68],\n",
    "  landmarks[69],\n",
    "  landmarks[70],\n",
    "  landmarks[71],\n",
    "  landmarks[72],\n",
    "  landmarks[73],\n",
    "  landmarks[74],\n",
    "  landmarks[75]], pix2mm)\n",
    "\n",
    "# Show results\n",
    "(h, w) = canvas.shape[:2]\n",
    "canvas = cv2.resize(canvas, (512, int(h*512/w)))\n",
    "cv2_imshow(canvas)\n",
    "\n",
    "# Print results\n",
    "print(f\"Right eye area: {right_eye_area} mm^2\")\n",
    "print(f\"Left eye area: {left_eye_area} mm^2\")\n",
    "d = abs(right_eye_area-left_eye_area)\n",
    "print(f\"Difference: {d} mm^2\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
