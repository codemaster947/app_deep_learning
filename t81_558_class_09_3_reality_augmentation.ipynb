{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZggjUZ5oPvzH"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_deep_learning/blob/main/t81_558_class_09_3_reality_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDTXd8-Lmp8Q"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "\n",
    "**Module 9: Facial Recognition**\n",
    "\n",
    "- Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "- For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncNrAEpzmp8S"
   },
   "source": [
    "# Module 9 Material\n",
    "\n",
    "- Part 9.1: Detecting Faces in an Image [[Video]](https://www.youtube.com/watch?v=Hpp3D3P2iWQ&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_09_1_faces.ipynb)\n",
    "- Part 9.2: Detecting Facial Features [[Video]](https://www.youtube.com/watch?v=AblTbq0T2wE&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_09_2_face_features.ipynb)\n",
    "- **Part 9.3: Reality Augmentation** [[Video]](https://www.youtube.com/watch?v=jfZDiRxx5Bc&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_09_3_reality_augmentation.ipynb)\n",
    "- Part 9.4: Application: Emotion Detection [[Video]](https://www.youtube.com/watch?v=F0H6vojQhE8&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_09_4_emotion.ipynb)\n",
    "- Part 9.5: Application: Blink Efficiency [[Video]](https://www.youtube.com/watch?v=96LPEStHCUA&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_09_5_blink.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKQqQljyPvzK"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code checks that Google CoLab is and sets up the correct hardware settings for PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fU9UhAxTmp8S",
    "outputId": "c6d1f18d-efe9-4436-a39a-3633c4899d53"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# Make use of a GPU or MPS (Apple) if one is available.  (see module 3.2)\n",
    "import torch\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "if device!='cuda':\n",
    "  print(\"*******WARNING, this notebook requires a CUDA GPU****\")\n",
    "  print(\"This notebook will not work correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-qb-mcqmp8U"
   },
   "source": [
    "# Part 9.3: Reality Augmentation\n",
    "\n",
    "\n",
    "Facial landmarks in computer vision refer to a set of key points on human faces, which are commonly used to deduce the structure and features of the face. These landmarks can be found around regions such as the eyes, nose, mouth, and jawline, serving as anchor points for various image processing tasks. Leveraging these facial landmarks, we can augment images by overlaying additional elements or graphics on the face. For instance, to create a Python program that draws eyeglasses on people, we would first detect the facial landmarks, particularly focusing on those around the eyes. Once these points are identified, we can determine the size, orientation, and position where the eyeglasses should be placed. Using libraries such as OpenCV, facenet and SPIGA, the program can then overlay a graphic of eyeglasses onto the original image in a manner that looks natural, effectively \"placing\" the eyeglasses on the person's face.\n",
    "\n",
    "We begin by installing SPIGA and Facenet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53sLzbmvxYSB",
    "outputId": "e75c780b-bb45-45b2-f7cd-eb95b57e9b91"
   },
   "outputs": [],
   "source": [
    "# Setup SPIGA and facenet\n",
    "!pip install facenet-pytorch\n",
    "!git clone https://github.com/andresprados/SPIGA.git\n",
    "%cd SPIGA/\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3S4PucjLfAPN"
   },
   "source": [
    "We are now ready to load an image to augment. The image is loaded as both a PIL and OpenCV form object. This duality is necessary because Facenet uses PIL format images; wehreas SPIGA uses OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JNuIn5cxeH6"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "#url = \"https://data.heatonresearch.com/images/wustl/data/img1-512.jpg\"\n",
    "url = \"https://s3.amazonaws.com/data.heatonresearch.com/images/jeff/about-jeff-heaton-2020.jpg\"\n",
    "\n",
    "# Download the image using requests\n",
    "response = requests.get(url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "response.raise_for_status()\n",
    "\n",
    "# Convert the downloaded bytes to a numpy array\n",
    "image = np.asarray(bytearray(response.content), dtype=\"uint8\")\n",
    "\n",
    "# Decode the numpy array to an OpenCV image\n",
    "img = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "\n",
    "# Convert the OpenCV image (NumPy array) to a PIL Image\n",
    "image_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "image_pil = Image.fromarray(image_rgb)\n",
    "\n",
    "mtcnn = MTCNN(keep_all=False, device=device)\n",
    "\n",
    "# Detect faces\n",
    "boxes, _ = mtcnn.detect(image_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRUrvJXZfRTy"
   },
   "source": [
    "Once we've extracted a face we use SPIGA to determine the facial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v3Dz9cg2xkHt",
    "outputId": "abd2fa1e-c1cf-49da-cb73-61be2e25b0c6"
   },
   "outputs": [],
   "source": [
    "from spiga.inference.config import ModelConfig\n",
    "from spiga.inference.framework import SPIGAFramework\n",
    "\n",
    "# Create a bounding box for the face we just detected.\n",
    "bbox = [\n",
    "    boxes[0][0],\n",
    "    boxes[0][1],\n",
    "    boxes[0][2],\n",
    "    boxes[0][3]]\n",
    "\n",
    "# Process image\n",
    "dataset = 'wflw'\n",
    "processor = SPIGAFramework(ModelConfig(dataset))\n",
    "features = processor.inference(img, [bbox])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jx51N3m1fStG"
   },
   "source": [
    "The provided code aims to overlay an eyeglasses graphic on top of an image of a face. The code starts by importing necessary modules. The image is duplicated using the **copy.deepcopy** method to create a canvas. This canvas will be modified to overlay the eyeglasses. The face's landmark points and headpose data are extracted from the features dictionary. These landmarks are crucial as they indicate key points on the face, such as the eyes, nose, and mouth.\n",
    "\n",
    "The **draw_lens** function is defined to illustrate the lens of the eyeglasses. This function uses the OpenCV library (cv2) to draw an elliptical shape representing a lens. The function is called twice: once for the left lens and once for the right lens, using specific landmarks for placement and size.\n",
    "\n",
    "A line is also drawn between the lenses, representing the bridge of the glasses. Finally, the canvas (image with the glasses overlay) is resized for a consistent width of 512 pixels while maintaining the original aspect ratio. The result is displayed using cv2_imshow, which is tailored for viewing images within Google Colab notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "TvlDJu5Rxoll",
    "outputId": "eec3644f-282f-4979-9797-b3c32941234e"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from spiga.demo.visualize.plotter import Plotter\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# Prepare variables\n",
    "\n",
    "\n",
    "canvas = copy.deepcopy(img)\n",
    "landmarks = np.array(features['landmarks'][0])\n",
    "headpose = np.array(features['headpose'][0])\n",
    "\n",
    "# Show image results\n",
    "\n",
    "def draw_lens(canvas,center,top,side):\n",
    "  print(center)\n",
    "  l1 = int(cv2.norm(center.astype(int),top.astype(int)))\n",
    "  l2 = int(cv2.norm(center.astype(int),side.astype(int)))\n",
    "  cv2.ellipse(canvas, center.astype(int), (l1,l2),\n",
    "            0, 0, 360, (255,0,0), 5)\n",
    "\n",
    "draw_lens(canvas,landmarks[96],landmarks[60],landmarks[62])\n",
    "draw_lens(canvas,landmarks[97],landmarks[68],landmarks[70])\n",
    "cv2.line(canvas,landmarks[68].astype(int),landmarks[64].astype(int),(255,0,0),5)\n",
    "\n",
    "\n",
    "(h, w) = canvas.shape[:2]\n",
    "canvas = cv2.resize(canvas, (512, int(h*512/w)))\n",
    "cv2_imshow(canvas)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
