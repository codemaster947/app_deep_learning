{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PF0TtkQ4-NoP"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_deep_learning/blob/main/t81_558_class_12_2_qlearningreinforcement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dkLTudD-NoQ"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 12: Reinforcement Learning**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wN-T0x2j-NoR"
   },
   "source": [
    "# Module 12 Video Material\n",
    "\n",
    "* Part 12.1: Introduction to Introduction to Gymnasium [[Video]](https://www.youtube.com/watch?v=FvuyrpzvwdI&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_1_reinforcement.ipynb)\n",
    "* **Part 12.2: Introduction to Q-Learning** [[Video]](https://www.youtube.com/watch?v=VKuqvbG_KAw&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_2_qlearningreinforcement.ipynb)\n",
    "* Part 12.3: Stable Baselines Q-Learning [[Video]](https://www.youtube.com/watch?v=kl7zsCjULN0&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_3_pytorch_reinforce.ipynb)\n",
    "* Part 12.4: Atari Games with Stable Baselines Neural Networks [[Video]](https://www.youtube.com/watch?v=maLA1_d4pzQ&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_4_atari.ipynb)\n",
    "* Part 12.5: Future of Reinforcement Learning [[Video]](https://www.youtube.com/watch?v=-euo5pTjP8E&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_5_rl_future.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIOgB0oG-NoS"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running the correct version of TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ERfMETL_-NoS",
    "outputId": "14ac4f5d-ce91-4e6b-d36f-19f23626d937"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IcTY1gkD-X5r",
    "outputId": "3d800245-3c10-4223-edfc-52688cc35bf4"
   },
   "outputs": [],
   "source": [
    "# HIDE OUTPUT\n",
    "if COLAB:\n",
    "  !pip install gymnasium[accept-rom-license,atari]\n",
    "  !pip install pyvirtualdisplay\n",
    "  !sudo apt-get install -y xvfb ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zA-E4NR2-NoY"
   },
   "source": [
    "# Part 12.2: Introduction to Q-Learning\n",
    "\n",
    "Q-Learning is a foundational technology upon which deep reinforcement learning is based. Before we explore deep reinforcement learning, it is essential to understand Q-Learning. Several components make up any Q-Learning system.\n",
    "\n",
    "* **Agent** - The agent is an entity that exists in an environment that takes actions to affect the state of the environment, to receive rewards.\n",
    "* **Environment** - The environment is the universe that the agent exists in. The environment is always in a specific state that is changed by the agent's actions.\n",
    "* **Actions** - Steps that the agent can perform to alter the environment\n",
    "* **Step** - A step occurs when the agent performs an action and potentially changes the environment state.\n",
    "* **Episode** - A chain of steps that ultimately culminates in the environment entering a terminal state.\n",
    "* **Epoch** - A training iteration of the agent that contains some number of episodes.\n",
    "* **Terminal State** -  A state in which further actions do not make sense. A terminal state occurs when the agent has one, lost, or the environment exceeds the maximum number of steps in many environments.\n",
    "\n",
    "Q-Learning works by building a table that suggests an action for every possible state. This approach runs into several problems. First, the environment is usually composed of several continuous numbers, resulting in an infinite number of states. Q-Learning handles continuous states by binning these numeric values into ranges.\n",
    "\n",
    "Out of the box, Q-Learning does not deal with continuous inputs, such as a car's accelerator that can range from released to fully engaged. Additionally, Q-Learning primarily deals with discrete actions, such as pressing a joystick up or down. Researchers have developed clever tricks to allow Q-Learning to accommodate continuous actions.\n",
    "\n",
    "Deep neural networks can help solve the problems of continuous environments and action spaces. In the next section, we will learn more about deep reinforcement learning. For now, we will apply regular Q-Learning to the Mountain Car problem from OpenAI Gym.\n",
    "\n",
    "## Introducing the Mountain Car\n",
    "\n",
    "This section will demonstrate how Q-Learning can create a solution to the mountain car gym environment. The Mountain car is an environment where a car must climb a mountain. Because gravity is stronger than the car's engine, it cannot merely accelerate up the steep slope even with full throttle. The vehicle is situated in a valley and must learn to utilize potential energy by driving up the opposite hill before the car can make it to the goal at the top of the rightmost hill.\n",
    "\n",
    "First, it might be helpful to visualize the mountain car environment. The following code shows this environment. This code makes use of TF-Agents to perform this render. Usually, we use TF-Agents for the type of deep reinforcement learning that we will see in the next module. However, TF-Agents is just used to render the mountain care environment for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "SkzXb-4kB0oP",
    "outputId": "569ee4c3-24be-41c6-b07b-8b26a1c8572e"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from PIL import Image\n",
    "\n",
    "# Create and initialize the MountainCar environment\n",
    "env = gym.make('MountainCar-v0', render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "\n",
    "# Render the environment's state to a numpy array\n",
    "frame = env.render()\n",
    "\n",
    "# Convert the numpy array to an image and display it\n",
    "image = Image.fromarray(frame)\n",
    "\n",
    "# Don't forget to close the environment when you're done!\n",
    "env.close()\n",
    "\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQBcs3yAMo_P"
   },
   "source": [
    "The mountain car environment provides the following discrete actions:\n",
    "\n",
    "* 0 - Apply left force\n",
    "* 1 - Apply no force\n",
    "* 2 - Apply right force\n",
    "\n",
    "The mountain car environment is made up of the following continuous values:\n",
    "\n",
    "* state[0] - Position\n",
    "* state[1] - Velocity\n",
    "\n",
    "The cart is not strong enough. It will need to use potential energy from the mountain behind it. The following code shows an agent that applies full throttle to climb the hill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wYTpj1_RigEv",
    "outputId": "94e33a3a-cb14-4817-b75b-2bf0644478c5"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gymnasium as gymnasium\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "import numpy as np\n",
    "\n",
    "# Start virtual display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "# Create Mountain Car environment\n",
    "env = gymnasium.make('MountainCar-v0', render_mode=\"rgb_array\")\n",
    "env.metadata['render_fps'] = 30\n",
    "# Reset the environment\n",
    "env.reset()\n",
    "\n",
    "# Setup the wrapper to record the video\n",
    "video_callable=lambda episode_id: True\n",
    "env = RecordVideo(env, video_folder='./videos', episode_trigger=video_callable)\n",
    "\n",
    "# Run the environment until done\n",
    "\n",
    "truncated = False\n",
    "i=0\n",
    "while not truncated:\n",
    "  i+=1\n",
    "  action = 2\n",
    "  state, reward, terminated, truncated , info = env.step(action)\n",
    "  print(f\"Step {i}: State={state}, Reward={reward}, term={terminated}, trunc={truncated}, info={info}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Display the video\n",
    "video = io.open(glob.glob('videos/*.mp4')[0], 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "ipythondisplay.display(HTML(data='''\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "    </video>\n",
    "'''.format(encoded.decode('ascii'))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVcdwI-0-Nod"
   },
   "source": [
    "## Programmed Car\n",
    "\n",
    "Now we will look at a car that I hand-programmed. This car is straightforward; however, it solves the problem. The programmed car always applies force in one direction or another. It does not break. Whatever direction the vehicle is currently rolling, the agent uses power in that direction. Therefore, the car begins to climb a hill, is overpowered, and turns backward. However, once it starts to roll backward, force is immediately applied in this new direction.\n",
    "\n",
    "The following code implements this preprogrammed car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AMzYrqRy-Noe",
    "outputId": "5d1d4898-e0b4-481e-a9a8-f00af827f3b2"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gymnasium as gymnasium\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "import numpy as np\n",
    "\n",
    "# Start virtual display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "# Create Mountain Car environment\n",
    "env = gymnasium.make('MountainCar-v0', render_mode=\"rgb_array\")\n",
    "env.metadata['render_fps'] = 30\n",
    "# Reset the environment\n",
    "env.reset()\n",
    "\n",
    "# Setup the wrapper to record the video\n",
    "video_callable=lambda episode_id: True\n",
    "env = RecordVideo(env, video_folder='./videos', episode_trigger=video_callable)\n",
    "\n",
    "# Run the environment until done\n",
    "\n",
    "truncated = False\n",
    "i=0\n",
    "action = 2\n",
    "while not truncated:\n",
    "  i+=1\n",
    "  state, reward, terminated, truncated , info = env.step(action)\n",
    "  print(f\"Step {i}: State={state}, Reward={reward}, term={terminated}, trunc={truncated}, info={info}\")\n",
    "  if state[1] > 0:\n",
    "    action = 2\n",
    "  else:\n",
    "    action = 0\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Display the video\n",
    "video = io.open(glob.glob('videos/*.mp4')[0], 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "ipythondisplay.display(HTML(data='''\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "    </video>\n",
    "'''.format(encoded.decode('ascii'))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAyqqoy9-Nog"
   },
   "source": [
    "### Reinforcement Learning\n",
    "\n",
    "Q-Learning is a system of rewards that the algorithm gives an agent for successfully moving the environment into a state considered successful. These rewards are the Q-values from which this algorithm takes its name. The final output from the Q-Learning algorithm is a table of Q-values that indicate the reward value of every action that the agent can take, given every possible environment state. The agent must bin continuous state values into a fixed finite number of columns.\n",
    "\n",
    "Learning occurs when the algorithm runs the agent and environment through episodes and updates the Q-values based on the rewards received from actions taken; Figure 12.REINF provides a high-level overview of this reinforcement or Q-Learning loop.\n",
    "\n",
    "**Figure 12.REINF:Reinforcement/Q Learning**\n",
    "![Reinforcement Learning](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/reinforcement.png \"Reinforcement Learning\")\n",
    "\n",
    "The Q-values can dictate action by selecting the action column with the highest Q-value for the current environment state. The choice between choosing a random action and a Q-value-driven action is governed by the epsilon ($\\epsilon$) parameter, the probability of random action.\n",
    "\n",
    "Each time through the training loop, the training algorithm updates the Q-values according to the following equation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzZnd9Tj7rNc"
   },
   "source": [
    "$Q^{new}(s_{t},a_{t}) \\leftarrow \\underbrace{Q(s_{t},a_{t})}_{\\text{old value}} + \\underbrace{\\alpha}_{\\text{learning rate}} \\cdot  \\overbrace{\\bigg( \\underbrace{\\underbrace{r_{t}}_{\\text{reward}} + \\underbrace{\\gamma}_{\\text{discount factor}} \\cdot \\underbrace{\\max_{a}Q(s_{t+1}, a)}_{\\text{estimate of optimal future value}}}_{\\text{new value (temporal difference target)}} - \\underbrace{Q(s_{t},a_{t})}_{\\text{old value}} \\bigg) }^{\\text{temporal difference}}$\n",
    "\n",
    "There are several parameters in this equation:\n",
    "* alpha ($\\alpha$) - The learning rate, how much should the current step cause the Q-values to be updated.\n",
    "* lambda ($\\lambda$) - The discount factor is the percentage of future reward that the algorithm should consider in this update.\n",
    "\n",
    "This equation modifies several values:\n",
    "\n",
    "* $Q(s_t,a_t)$ - The Q-table.  For each combination of states, what reward would the agent likely receive for performing each action?\n",
    "* $s_t$ - The current state.\n",
    "* $r_t$ - The last reward received.\n",
    "* $a_t$ - The action that the agent will perform.\n",
    "\n",
    "The equation works by calculating a delta (temporal difference) that the equation should apply to the old state. This learning rate ($\\alpha$) scales this delta. A learning rate of 1.0 would fully implement the temporal difference in the Q-values each iteration and would likely be very chaotic.\n",
    "\n",
    "There are two parts to the temporal difference: the new and old values. The new value is subtracted from the old value to provide a delta; the full amount we would change the Q-value by if the learning rate did not scale this value. The new value is a summation of the reward received from the last action and the maximum Q-values from the resulting state when the client takes this action. Adding the maximum of action Q-values for the new state is essential because it estimates the optimal future values from proceeding with this action.\n",
    "\n",
    "## Q-Learning Car\n",
    "\n",
    "We will now use Q-Learning to produce a car that learns to drive itself. Look out, Tesla! We begin by defining two essential functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UrQgXqbS-Noi",
    "outputId": "68ef0118-a5eb-450c-c0a1-5666fe59fec6"
   },
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import numpy as np\n",
    "\n",
    "# This function converts the floating point state values into\n",
    "# discrete values. This is often called binning.  We divide\n",
    "# the range that the state values might occupy and assign\n",
    "# each region to a bucket.\n",
    "def calc_discrete_state(state):\n",
    "    discrete_state = (state - env.observation_space.low)/buckets\n",
    "    return tuple(discrete_state.astype(int))\n",
    "\n",
    "# Run one game.  The q_table to use is provided.  We also\n",
    "# provide a flag to indicate if the game should be\n",
    "# rendered/animated.  Finally, we also provide\n",
    "# a flag to indicate if the q_table should be updated.\n",
    "def run_game(q_table, render, should_update):\n",
    "    done = False\n",
    "    discrete_state = calc_discrete_state(env.reset()[0])\n",
    "    success = False\n",
    "\n",
    "    while not done:\n",
    "        # Exploit or explore\n",
    "        if np.random.random() > epsilon:\n",
    "            # Exploit - use q-table to take current best action\n",
    "            # (and probably refine)\n",
    "            action = np.argmax(q_table[discrete_state])\n",
    "        else:\n",
    "            # Explore - t\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "\n",
    "        # Run simulation step\n",
    "        new_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # Convert continuous state to discrete\n",
    "        new_state_disc = calc_discrete_state(new_state)\n",
    "\n",
    "        # Have we reached the goal position (have we won?)?\n",
    "        if new_state[0] >= env.unwrapped.goal_position:\n",
    "            success = True\n",
    "\n",
    "        # Update q-table\n",
    "        if should_update:\n",
    "            max_future_q = np.max(q_table[new_state_disc])\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * \\\n",
    "                (reward + DISCOUNT * max_future_q)\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "        discrete_state = new_state_disc\n",
    "\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "    return success\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3j1M4MddIBF"
   },
   "source": [
    "Several hyperparameters are very important for Q-Learning. These parameters will likely need adjustment as you apply Q-Learning to other problems. Because of this, it is crucial to understand the role of each parameter.\n",
    "\n",
    "* **LEARNING_RATE** The rate at which previous Q-values are updated based on new episodes run during training.\n",
    "* **DISCOUNT** The amount of significance to give estimates of future rewards when added to the reward for the current action taken. A value of 0.95 would indicate a discount of 5% on the future reward estimates.\n",
    "* **EPISODES** The number of episodes to train over. Increase this for more complex problems; however, training time also increases.\n",
    "* **SHOW_EVERY** How many episodes to allow to elapse before showing an update.\n",
    "* **DISCRETE_GRID_SIZE** How many buckets to use when converting each continuous state variable. For example, [10, 10] indicates that the algorithm should use ten buckets for the first and second state variables.\n",
    "* **START_EPSILON_DECAYING** Epsilon is the probability that the agent will select a random action over what the Q-Table suggests. This value determines the starting probability of randomness.\n",
    "* **END_EPSILON_DECAYING** How many episodes should elapse before epsilon goes to zero and no random actions are permitted. For example, EPISODES//10  means only the first 1/10th of the episodes might have random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rI0TJc7T-Nol"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 500\n",
    "SHOW_EVERY = 100\n",
    "\n",
    "DISCRETE_GRID_SIZE = [10, 10]\n",
    "START_EPSILON_DECAYING = 0.5\n",
    "END_EPSILON_DECAYING = EPISODES//10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rw3SjzSyAktT"
   },
   "source": [
    "We can now make the environment.  If we are running in Google COLAB, we wrap the environment to be displayed inside the web browser.  Next, create the discrete buckets for state and build Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2IdL-46y-Noo"
   },
   "outputs": [],
   "source": [
    "env = gymnasium.make(\"MountainCar-v0\")\n",
    "\n",
    "epsilon = 1\n",
    "epsilon_change = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
    "buckets = (env.observation_space.high - env.observation_space.low) \\\n",
    "    / DISCRETE_GRID_SIZE\n",
    "q_table = np.random.uniform(low=-3, high=0, size=(DISCRETE_GRID_SIZE\n",
    "                                                  + [env.action_space.n]))\n",
    "success = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2qG3ayDsRWz"
   },
   "source": [
    "We can now make the environment.  If we are running in Google COLAB, we wrap the environment to be displayed inside the web browser.  Next, create the discrete buckets for state and build Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ot7cF0bX-Nor",
    "outputId": "b32cadb3-3f74-48fb-a5cf-2717e3e01648"
   },
   "outputs": [],
   "source": [
    "episode = 0\n",
    "success_count = 0\n",
    "\n",
    "# Loop through the required number of episodes\n",
    "while episode < EPISODES:\n",
    "    episode += 1\n",
    "    done = False\n",
    "\n",
    "    # Run the game.  If we are local, display render animation\n",
    "    # at SHOW_EVERY intervals.\n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        print(f\"Current episode: {episode}, success: {success_count}\" +\n",
    "              f\" {(float(success_count)/SHOW_EVERY)}\")\n",
    "        success = run_game(q_table, True, False)\n",
    "        success_count = 0\n",
    "    else:\n",
    "        success = run_game(q_table, False, True)\n",
    "\n",
    "    # Count successes\n",
    "    if success:\n",
    "        success_count += 1\n",
    "\n",
    "    # Move epsilon towards its ending value, if it still needs to move\n",
    "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
    "        epsilon = max(0, epsilon - epsilon_change)\n",
    "\n",
    "print(success)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDHTQkREFRSE"
   },
   "source": [
    "As you can see, the number of successful episodes generally increases as training progresses. It is not advisable to stop the first time we observe 100% success over 1,000 episodes. There is a randomness to most games, so it is not likely that an agent would retain its 100% success rate with a new run. It might be safe to stop training once you observe that the agent has gotten 100% for several update intervals.\n",
    "\n",
    "## Running and Observing the Agent\n",
    "\n",
    "Now that the algorithm has trained the agent, we can observe the agent in action. You can use the following code to see the agent in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "v_mf3A0h-Nox",
    "outputId": "c2df1204-12a9-4a3b-c60f-c68ea217c7c8"
   },
   "outputs": [],
   "source": [
    "# HIDE OUTPUT\n",
    "\n",
    "# Setup the wrapper to record the video\n",
    "env = gymnasium.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
    "video_callable=lambda episode_id: True\n",
    "env = RecordVideo(env, video_folder='./videos', episode_trigger=video_callable)\n",
    "\n",
    "run_game(q_table, True, False)\n",
    "\n",
    "# Display the video\n",
    "video = io.open(glob.glob('videos/*.mp4')[0], 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "ipythondisplay.display(HTML(data='''\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "    </video>\n",
    "'''.format(encoded.decode('ascii'))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeQzNFSosdys"
   },
   "source": [
    "## Inspecting the Q-Table\n",
    "\n",
    "We can also display the Q-table. The following code shows the agent's action for each environment state. As the weights of a neural network, this table is not straightforward to interpret. Some patterns do emerge in that direction, as seen by calculating the means of rows and columns. The actions seem consistent at both velocity and position's upper and lower halves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "L62lyCTr-Noz",
    "outputId": "4be0cc48-8686-4bae-ec6b-0064cc950352"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(q_table.argmax(axis=2))\n",
    "\n",
    "df.columns = [f'v-{x}' for x in range(DISCRETE_GRID_SIZE[0])]\n",
    "df.index = [f'p-{x}' for x in range(DISCRETE_GRID_SIZE[1])]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z-ZOL-d5Jbon",
    "outputId": "fa5c9181-8842-4be3-b531-765135223c24"
   },
   "outputs": [],
   "source": [
    "df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LOGA4HP9KHW3",
    "outputId": "948b21dc-b66e-44a3-a6f7-942f929a1f17"
   },
   "outputs": [],
   "source": [
    "df.mean(axis=1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
