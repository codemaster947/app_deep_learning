{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZggjUZ5oPvzH"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_deep_learning/blob/main/t81_558_class_09_4_emotion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDTXd8-Lmp8Q"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "\n",
    "**Module 9: Facial Recognition**\n",
    "\n",
    "- Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "- For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncNrAEpzmp8S"
   },
   "source": [
    "# Module 9 Material\n",
    "\n",
    "- Part 9.1: Detecting Faces in an Image [[Video]](https://www.youtube.com/watch?v=Hpp3D3P2iWQ&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_09_1_faces.ipynb)\n",
    "- Part 9.2: Detecting Facial Features [[Video]](https://www.youtube.com/watch?v=AblTbq0T2wE&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_09_2_face_features.ipynb)\n",
    "- Part 9.3: Reality Augmentation [[Video]](https://www.youtube.com/watch?v=jfZDiRxx5Bc&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_09_3_reality_augmentation.ipynb)\n",
    "- **Part 9.4: Application: Emotion Detection** [[Video]](https://www.youtube.com/watch?v=F0H6vojQhE8&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_09_4_emotion.ipynb)\n",
    "- Part 9.5: Application: Blink Efficiency [[Video]](https://www.youtube.com/watch?v=96LPEStHCUA&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_09_5_blink.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKQqQljyPvzK"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code checks that Google CoLab is and sets up the correct hardware settings for PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fU9UhAxTmp8S",
    "outputId": "b22b40b7-7315-49a6-d971-f24d662c9d68"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-qb-mcqmp8U"
   },
   "source": [
    "# Part 9.4: Application: Emption Detection\n",
    "\n",
    "The Python **fer** package stands for \"Facial Emotion Recognition\" and is a potent tool that harnesses the capabilities of computer vision to detect emotions in people's faces. By analyzing facial features and expressions, the package can identify a range of emotions, such as happiness, sadness, surprise, and anger, among others. Leveraging this technology has wide-reaching applications. For instance, marketers can use it to gauge real-time reactions of consumers to advertisements or new product launches. In the realm of entertainment, filmmakers and game developers can adjust content based on the viewers' emotional feedback. Additionally, in the healthcare sector, it can assist in monitoring patients' emotional well-being. As we delve deeper, we will be exploring how to employ the **fer** package to tap into these vast opportunities and detect emotions effectively.\n",
    "\n",
    "We begin by installing the *fer** package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9kGeXys_fW3k",
    "outputId": "8cc5e13b-4978-4596-9632-6fbbb355c006"
   },
   "outputs": [],
   "source": [
    "!pip install fer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A67jCkKcyLlh"
   },
   "source": [
    "We begin by loading an image and displaying it. We utilize OpenCV to both load and display this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "l1FE7CwGf4Io",
    "outputId": "10f81368-d0cf-4a8b-cb6c-a4a966082fd7"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from facenet_pytorch import MTCNN\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "url = \"https://data.heatonresearch.com/images/jeff/about-jeff-heaton-2020.jpg\"\n",
    "\n",
    "# Download the image using requests\n",
    "response = requests.get(url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "response.raise_for_status()\n",
    "\n",
    "# Convert the downloaded bytes to a numpy array\n",
    "image = np.asarray(bytearray(response.content), dtype=\"uint8\")\n",
    "\n",
    "# Decode the numpy array to an OpenCV image\n",
    "img = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "\n",
    "# Display\n",
    "(h, w) = img.shape[:2]\n",
    "d = cv2.resize(img, (512, int(h*512/w)))\n",
    "cv2_imshow(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgGF-IzW-8r8"
   },
   "source": [
    "We are now ready to make use of **fer** to interpret the emption of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qgAHgIbIgIEc",
    "outputId": "aa1dc93d-a38a-46df-a711-38e18801d8fc"
   },
   "outputs": [],
   "source": [
    "from fer import FER\n",
    "import cv2\n",
    "\n",
    "detector = FER()\n",
    "detector.detect_emotions(img)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRP4XUu2_CkX"
   },
   "source": [
    "We can easily try a different image to see what emotion is detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "AUL0bCJ-3csK",
    "outputId": "45b49257-efa5-46e8-d97a-b8ea6a635815"
   },
   "outputs": [],
   "source": [
    "url = \"https://data.heatonresearch.com/images/wustl/data/jeff-emotion.jpg\"\n",
    "\n",
    "# Download the image using requests\n",
    "response = requests.get(url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "response.raise_for_status()\n",
    "\n",
    "# Convert the downloaded bytes to a numpy array\n",
    "image = np.asarray(bytearray(response.content), dtype=\"uint8\")\n",
    "\n",
    "# Decode the numpy array to an OpenCV image\n",
    "img = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "\n",
    "# Display\n",
    "(h, w) = img.shape[:2]\n",
    "d = cv2.resize(img, (512, int(h*512/w)))\n",
    "cv2_imshow(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xxndt-Dh_mND"
   },
   "source": [
    "Here we see a mix of fear and surprise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tML1UvJy6tZv",
    "outputId": "85987669-703d-4414-a8f3-abbf4b81c280"
   },
   "outputs": [],
   "source": [
    "from fer import FER\n",
    "import cv2\n",
    "\n",
    "detector = FER()\n",
    "detector.detect_emotions(img)[0]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
