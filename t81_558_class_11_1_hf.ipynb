{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CF2edFAI4Uj"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_deep_learning/blob/master/t81_558_class_11_1_hf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvbM-RwHI4Ul"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 11: Natural Language Processing with Hugging Face**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dE7A-0aI4Ul"
   },
   "source": [
    "# Module 11 Material\n",
    "\n",
    "* **Part 11.1: Introduction to Hugging Face** [[Video]](https://www.youtube.com/watch?v=PzuL84ksRuE&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_11_1_hf.ipynb)\n",
    "* Part 11.2: Hugging Face in Python [[Video]](https://www.youtube.com/watch?v=tkGIF4CFoV4&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_11_2_py_huggingface.ipynb)\n",
    "* Part 11.3: Hugging Face Tokenizers [[Video]](https://www.youtube.com/watch?v=Cz2nvfK28eI&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_11_3_tokenizers.ipynb)\n",
    "* Part 11.4: Hugging Face Datasets [[Video]](https://www.youtube.com/watch?v=yLlCZLzE2XU&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_11_4_hf_datasets.ipynb)\n",
    "* Part 11.5: Training Hugging Face Models [[Video]](https://www.youtube.com/watch?v=7YZOik5S3vs&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_11_5_hf_train.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Z4A091yI4Um"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code checks if Google CoLab is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJmzbge9I4Um",
    "outputId": "f98a353c-05b4-4678-8b99-9dfae65e673e"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdNN6e45I4Un"
   },
   "source": [
    "# Part 11.1: Introduction to Hugging Face\n",
    "\n",
    "Hugging Face is a company renowned for its pioneering work in the realm of Natural Language Processing (NLP). Established in 2016, Hugging Face has become synonymous with making cutting-edge NLP technologies more accessible to developers, researchers, and organizations. The company's commitment to open-source development has fostered a vibrant community, which plays a pivotal role in the rapid advancements of the field.\n",
    "\n",
    "Central to Hugging Face's acclaim is the Transformers library. This Python library provides an extensive collection of pre-trained models that span a wide range of NLP tasks, from text classification and tokenization to language modeling and translation. Some of the most groundbreaking models like BERT, GPT-2, T5, and RoBERTa can be effortlessly accessed and deployed through this library.\n",
    "\n",
    "A notable feature of the Transformers library is its user-friendly API. With just a few lines of code, one can:\n",
    "\n",
    "* Load a pre-trained model.\n",
    "* Tokenize input text.\n",
    "* Obtain model predictions or embeddings.\n",
    "* Fine-tune the model on a specific task.\n",
    "\n",
    "This ease of use, combined with comprehensive documentation and tutorials, makes it an invaluable tool for both NLP newcomers and seasoned professionals.\n",
    "\n",
    "Another significant contribution from Hugging Face is the Model Hub, a platform where researchers and developers can share, discover, and use NLP models. The hub promotes collaboration, ensuring that state-of-the-art models are easily available to the wider community. It's not uncommon to see models from recent research papers promptly available on the hub, ready for real-world applications.\n",
    "\n",
    "Tokenization, the process of converting text into tokens, is a fundamental step in NLP. Recognizing its importance, Hugging Face introduced the Tokenizers library, which provides a fast and efficient way to tokenize massive datasets without compromising on accuracy. Its compatibility with the Transformers library ensures seamless integration between tokenization and modeling.\n",
    "\n",
    "Democratization of NLP: By making advanced models and tools accessible, Hugging Face has democratized NLP, enabling even small teams or individual developers to harness the power of state-of-the-art models.\n",
    "\n",
    "Rapid Prototyping: The ease with which one can deploy models means that ideas can be tested and iterated upon swiftly, accelerating the pace of NLP advancements.\n",
    "\n",
    "Community and Collaboration: The open-source ethos of Hugging Face has cultivated a community that collaborates, contributes, and ensures that the field remains vibrant and progressive.\n",
    "\n",
    "In the ever-evolving landscape of NLP, Hugging Face stands out as a beacon of innovation, accessibility, and collaboration. Whether you are a researcher pushing the boundaries of what's possible, a developer aiming to integrate NLP into an application, or an enthusiast eager to learn, Hugging Face provides the tools and resources to realize those ambitions. As we delve deeper into this book, we will frequently use the Hugging Face platform to illustrate concepts, implement solutions, and explore the vast possibilities of NLP.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of t81_558_class_11_01_huggingface.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
