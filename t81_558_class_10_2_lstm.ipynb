{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdSnFaggnD2W"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_deep_learning/blob/main/t81_558_class_10_2_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gc4uQ-bYnD2Y"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 10: Time Series in PyTorch**  \n",
    "\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFj4U9c8nD2Y"
   },
   "source": [
    "# Module 10 Material\n",
    "\n",
    "* Part 10.1: Time Series Data Encoding for Deep Learning, PyTorch** [[Video]](https://www.youtube.com/watch?v=CZi5Avp6p1s&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_10_1_timeseries.ipynb)\n",
    "* **Part 10.2: LSTM-Based Time Series with PyTorch** [[Video]](https://www.youtube.com/watch?v=hIQLy5zCgH4&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_10_2_lstm.ipynb)\n",
    "* Part 10.3: Transformer-Based Time Series with PyTorch [[Video]](https://www.youtube.com/watch?v=NGzQpphf_Vc&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_10_3_transformer_timeseries.ipynb)\n",
    "* Part 10.4: Seasonality and Trend [[Video]](https://www.youtube.com/watch?v=HOkxoLaUF9s&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_10_4_seasonal.ipynb)\n",
    "* Part 10.5: Predicting with Meta Prophet [[Video]](https://www.youtube.com/watch?v=MzjMVsz0GyA&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_10_5_prophet.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MtBKuaZnD2Z"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code checks that Google CoLab is and sets up the correct hardware settings for PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nT6Wli6knD2Z",
    "outputId": "9819d7d3-66b8-4aba-cdf2-9af14f25ff0e"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# Make use of a GPU or MPS (Apple) if one is available.  (see module 3.2)\n",
    "import torch\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RH3-887nD2a"
   },
   "source": [
    "# Part 10.2: LSTM-Based Time Series with PyTorch\n",
    "\n",
    "So far, the neural networks that we’ve examined have always had forward connections. Neural networks of this type always begin with an input layer connected to the first hidden layer. Each hidden layer always connects to the next hidden layer. The final hidden layer always connects to the output layer. This manner of connection is why these networks are called “feedforward.”  Recurrent neural networks are not as rigid, as backward linkages are also allowed. A recurrent connection links a neuron in a layer to either a previous layer or the neuron itself. Most recurrent neural network architectures maintain the state in the recurrent connections. Feedforward neural networks don’t keep any state.\n",
    "\n",
    "## Understanding LSTM\n",
    "\n",
    "Long Short Term Memory (LSTM) layers are a type of recurrent unit that you often use with deep neural networks.[[Cite:hochreiter1997long]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320) For TensorFlow, you can think of LSTM as a layer type that you can combine with other layer types, such as dense. LSTM makes use of two transfer function types internally.  \n",
    "\n",
    "The first type of transfer function is the sigmoid.  This transfer function type is used form gates inside of the unit.  The sigmoid transfer function is given by the following equation:\n",
    "\n",
    "$ \\mbox{S}(t) = \\frac{1}{1 + e^{-t}} $\n",
    "\n",
    "The second type of transfer function is the hyperbolic tangent (tanh) function, which allows you to scale the output of the LSTM. This functionality is similar to how we have used other transfer functions in this course.  \n",
    "\n",
    "We provide the graphs for these functions here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 879
    },
    "id": "aK4KCNPEnD2a",
    "outputId": "bd0b7388-b2b3-490e-a2a7-8ecc4c1be184"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    a = []\n",
    "    for item in x:\n",
    "        a.append(1/(1+math.exp(-item)))\n",
    "    return a\n",
    "\n",
    "def f2(x):\n",
    "    a = []\n",
    "    for item in x:\n",
    "        a.append(math.tanh(item))\n",
    "    return a\n",
    "\n",
    "x = np.arange(-10., 10., 0.2)\n",
    "y1 = sigmoid(x)\n",
    "y2 = f2(x)\n",
    "\n",
    "print(\"Sigmoid\")\n",
    "plt.plot(x,y1)\n",
    "plt.show()\n",
    "\n",
    "print(\"Hyperbolic Tangent(tanh)\")\n",
    "plt.plot(x,y2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyCPwHyfnD2b"
   },
   "source": [
    "Both of these two functions compress their output to a specific range.  For the sigmoid function, this range is 0 to 1.  For the hyperbolic tangent function, this range is -1 to 1.\n",
    "\n",
    "LSTM maintains an internal state and produces an output.  The following diagram shows an LSTM unit over three timeslices: the current time slice (t), as well as the previous (t-1) and next (t+1) slice, as demonstrated by Figure 10.LSTM.\n",
    "\n",
    "**Figure 10.LSTM: LSTM Layers**\n",
    "![LSTM Layers](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_10_lstm1.png \"LSTM Layers\")\n",
    "\n",
    "The values $\\hat{y}$ are the output from the unit; the values ($x$) are the input to the unit, and the values $c$ are the context values.  The output and context values always feed their output to the next time slice.  The context values allow the network to maintain the state between calls.  Figure 10.ILSTM shows the internals of a LSTM layer.\n",
    "\n",
    "**Figure 10.ILSTM: Inside a LSTM Layer**\n",
    "![LSTM Layers](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_10_lstm2.png \"Inside the LSTM\")\n",
    "\n",
    "A LSTM unit consists of three gates:\n",
    "\n",
    "* Forget Gate ($f_t$) - Controls if/when the context is forgotten. (MC)\n",
    "* Input Gate ($i_t$) - Controls if/when the context should remember a value. (M+/MS)\n",
    "* Output Gate ($o_t$) - Controls if/when the remembered value is allowed to pass from the unit. (RM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0mVhDjAnD2c"
   },
   "source": [
    "## Simple LSTM Example\n",
    "\n",
    "The following code creates the LSTM network, an example of an RNN for classification.  The following code trains on a data set (x) with a max sequence size of 6 (columns) and six training elements (rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-lBrvaWKnD2c",
    "outputId": "37ee36c1-a820-4a8d-b5a7-e0d65f6bd311"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Data\n",
    "max_features = 4\n",
    "x_data = [\n",
    "    [[0], [1], [1], [0], [0], [0]],\n",
    "    [[0], [0], [0], [2], [2], [0]],\n",
    "    [[0], [0], [0], [0], [3], [3]],\n",
    "    [[0], [2], [2], [0], [0], [0]],\n",
    "    [[0], [0], [3], [3], [0], [0]],\n",
    "    [[0], [0], [0], [0], [1], [1]]\n",
    "]\n",
    "x = torch.tensor(x_data, dtype=torch.float32)\n",
    "y = torch.tensor([1, 2, 3, 2, 3, 1], dtype=torch.int64)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y2 = torch.nn.functional.one_hot(y, max_features).to(torch.float32)\n",
    "print(y2)\n",
    "\n",
    "# Model using a sequence\n",
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMLayer, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return out\n",
    "\n",
    "model = nn.Sequential(\n",
    "    LSTMLayer(input_size=1, hidden_size=128),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(128*6, 4),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Check for GPU availability\n",
    "model.to(device)\n",
    "x, y2 = x.to(device), y2.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "print('Train...')\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, y2)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/200], Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# Predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(x)\n",
    "    predicted_classes = torch.argmax(outputs, 1)\n",
    "    print(f\"Predicted classes: {predicted_classes.cpu().numpy()}\")\n",
    "    print(f\"Expected classes: {y.cpu().numpy()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMIZvStnnD2c"
   },
   "source": [
    "We can now present a sequence directly to the model for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F3otdNdwnD2d",
    "outputId": "86763263-1ad2-4cd3-8487-df183e86db29"
   },
   "outputs": [],
   "source": [
    "def runit(model, inp):\n",
    "    inp = torch.tensor(inp, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inp)\n",
    "    return torch.argmax(outputs[0]).item()\n",
    "\n",
    "print(runit(model, [[[0], [0], [0], [0], [0], [1]]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShrkldWxnKFo"
   },
   "source": [
    "## Sun Spots Example\n",
    "\n",
    "This section shows an example of RNN regression to predict sunspots.  You can find the data files needed for this example at the following location.\n",
    "\n",
    "* [Sunspot Data Files](http://www.sidc.be/silso/datafiles#total)\n",
    "* [Download Daily Sunspots](http://www.sidc.be/silso/INFO/sndtotcsv.php) - 1/1/1818 to now.\n",
    "\n",
    "We begin by loading and preparing data for the LSTM model. Next, we define a list of the column headers for a dataset. Following that, we read a CSV file from the given URL using the **pd.read_csv* function. This dataset is sunspot activity. The CSV is provided by the USA government and has certain specifications:\n",
    "\n",
    "* It uses a semicolon (;) as a separator.\n",
    "* The dataset doesn't have a header, so the header=None argument ensures pandas doesn't mistakenly take the first row as column names. Instead, the predefined names list is used as the columns' header.\n",
    "* Any value of '-1' in the dataset is considered as a missing value (na_values=['-1']).\n",
    "* The dataset is read without setting an index column (index_col=False), meaning the default integer index will be used.\n",
    "* Once executed, this code will load the specified dataset into a pandas DataFrame named df.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OoH-IH_bELLs"
   },
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "names = ['year', 'month', 'day', 'dec_year', 'sn_value',\n",
    "         'sn_error', 'obs_num', 'unused1']\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/SN_d_tot_V2.0.csv\",\n",
    "    sep=';', header=None, names=names,\n",
    "    na_values=['-1'], index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li2i8ygFn7oc"
   },
   "source": [
    "Next we perform data preprocessing tasks for the sunspot dataset. Initially, the code identifies the last occurrence where the 'obs_num' column has a value, this is to strip off incomplete data near the beginning of the file. The **sn_value** column of the dataframe is then converted into floating-point numbers, ensuring numerical computations are consistent. After this, the dataframe is split into two parts based on the 'year' column: any data prior to the year 2000 is assigned to **df_train**, and data from the year 2000 onwards is assigned to **df_test**. Subsequently, the **sn_value** column from both the training and testing dataframes is extracted, transformed into numpy arrays, and reshaped to form 2D arrays with a single column.\n",
    "\n",
    "This restructuring is done to meet the input requirements of PyTorch. The final segment of the code focuses on data normalization. A StandardScaler is initialized, which is a tool to standardize data to have a mean of 0 and a standard deviation of 1. This scaler is trained on the training data and then used to normalize both the training and testing data. After normalization, both datasets are flattened and converted to lists, resulting in one-dimensional lists of normalized **sn_value** data for both training and testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwQ5JVNWmZFk"
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "start_id = max(df[df['obs_num'] == 0].index.tolist()) + 1\n",
    "df = df[start_id:].copy()\n",
    "df['sn_value'] = df['sn_value'].astype(float)\n",
    "df_train = df[df['year'] < 2000]\n",
    "df_test = df[df['year'] >= 2000]\n",
    "\n",
    "spots_train = df_train['sn_value'].to_numpy().reshape(-1, 1)\n",
    "spots_test = df_test['sn_value'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "spots_train = scaler.fit_transform(spots_train).flatten().tolist()\n",
    "spots_test = scaler.transform(spots_test).flatten().tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "popojYcVpiDE"
   },
   "source": [
    "The following code prepars the sequence data. This must be done in tasks like time series prediction or sequential data processing. The primary goal is to transform a list of observations into overlapping sequences of a specified length.\n",
    "\n",
    "The constant **SEQUENCE_SIZE** is set to 10, meaning that each sequence (or window) will consist of 10 observations.\n",
    "\n",
    "The function **to_sequences** is defined to facilitate this transformation. This function takes in two arguments: the size of each sequence (seq_size) and the list of observations (obs). Within the function, two empty lists, **x** and **y**, are initialized. Iterating over the **obs** list, for every index **i**, a window of size **seq_size** is extracted from **obs** and appended to the **x** list. The observation immediately following this window, or the (i + seq_size)-th observation, is appended to the **y** list. Essentially, **x** contains the sequences, and **y** contains the observations immediately following each sequence that are to be predicted. Once the lists are filled, they're converted into PyTorch tensors with the appropriate shapes and data type (torch.float32).\n",
    "\n",
    "Using this **to_sequences** function, the previously prepared **spots_train** and **spots_test** lists are transformed into their corresponding sequence datasets: **x_train**, **y_train**, **x_test**, and **y_test**. In this setup, if we consider **x_train** and **x_test**, each entry will represent a sequence of 10 observations, and the corresponding entry in **y_train** or **y_test** will represent the observation immediately following that sequence. This structure is particularly useful for tasks like predicting the next value in a time series based on a sequence of previous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVoIjklkmdDv"
   },
   "outputs": [],
   "source": [
    "# Sequence Data Preparation\n",
    "SEQUENCE_SIZE = 10\n",
    "\n",
    "def to_sequences(seq_size, obs):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(obs) - seq_size):\n",
    "        window = obs[i:(i + seq_size)]\n",
    "        after_window = obs[i + seq_size]\n",
    "        x.append(window)\n",
    "        y.append(after_window)\n",
    "    return torch.tensor(x, dtype=torch.float32).view(-1, seq_size, 1), torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_train, y_train = to_sequences(SEQUENCE_SIZE, spots_train)\n",
    "x_test, y_test = to_sequences(SEQUENCE_SIZE, spots_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoalnjZPqmlX"
   },
   "source": [
    "Next we set up data loaders for PyTorch, a crucial step when training neural networks in batches. First, the training data (**x_train** and **y_train**) is encapsulated into a TensorDataset. This structure pairs input data and its corresponding target, making it easier to manage. Similarly, the testing data (**x_test** and **y_test**) is also wrapped into a **TensorDataset**. Once the datasets are structured, they are passed to the **DataLoader** function.\n",
    "\n",
    "For the training data, a **DataLoader** is created with a batch size of 32, and the shuffle parameter is set to True, which means tha during each epoch of training, the training data will be divided into batches of 32 samples, and these batches will be randomly shuffled. This shuffling is to ensure the model isn't exposed to any inherent order in the data during training, promoting better generalization.\n",
    "\n",
    "Conversely, for the testing data, while the batch size remains 32, the shuffle parameter is set to False, indicating that the order of the test data remains unchanged. This is typical as shuffling the test data isn't necessary and can often make evaluation metrics easier to interpret. By the end of this code, two data loaders (train_loader and test_loader) are established, ready to feed data in batches to a neural network during both training and evaluation phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "449lnEVlmgki"
   },
   "outputs": [],
   "source": [
    "# Setup data loaders for batch\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbhQn0SovB_U"
   },
   "source": [
    "\n",
    "We define an **LSTMModel** class, which is a neural network architecture using PyTorch that provides more precise control than a sequence. This model is structured with an LSTM layer followed by a dropout layer for regularization and two subsequent fully connected layers. The LSTM layer processes input sequences, producing a series of hidden states. The model then utilizes only the last hidden state of this series, passing it through the dropout layer and the two linear layers in sequence. This architecture is geared towards processing sequential data, where the LSTM can capture temporal dependencies and the fully connected layers further refine the representation for the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRhEO-hwmj5D"
   },
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=64, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(x[:, -1, :])\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = LSTMModel().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4G3DvX6vq77"
   },
   "source": [
    "\n",
    "The code outlines the training process for a neural network model. It sets up a mean squared error (MSE) loss function and uses the Adam optimizer with a learning rate of 0.001. The learning rate scheduler, **ReduceLROnPlateau**, adjusts the learning rate when the validation loss plateaus, decreasing it by a factor of 0.5 if there's no improvement for three epochs. The model trains for a maximum of 1000 epochs but incorporates early stopping; if the validation loss doesn't improve for five consecutive epochs, the training halts prematurely. During each epoch, the model's weights are updated using the training data. Subsequently, the model's performance is evaluated on the validation data, and the average validation loss is computed. The progress of the training, including the current epoch and corresponding validation loss, is printed to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVPwS7EdmnO5",
    "outputId": "97faca55-8f23-49a3-937e-2b176931566f"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "epochs = 1000\n",
    "early_stop_count = 0\n",
    "min_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        x_batch, y_batch = batch\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x_batch, y_batch = batch\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    val_loss = np.mean(val_losses)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "\n",
    "    if early_stop_count >= 5:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8h1ZJOQwUAX"
   },
   "source": [
    "Finally, we evaluate the PyTorch neural network model on test data. It initiates by setting the model to evaluation mode using **model.eval**, which ensures that specific layers like dropout are fixed during inference. The predictions list is initialized to store the model's predictions on the test data. The **torch.no_grad** context is used to disable gradient calculations, optimizing memory usage and speed during evaluation. Inside this context, the code iterates over the **test_loader** to fetch batches of test data.\n",
    "\n",
    "Each data batch is then transferred to the computing device, and the model is subsequently used to generate predictions on this batch. These predictions are added to the predictions list. After processing all test batches, the code calculates the Root Mean Square Error (RMSE) between the predicted values and the actual targets (**y_test**). Notably, the predictions and targets are inverse-transformed using the scaler to revert the normalization and compute the RMSE in the original data scale. The computed RMSE, a measure of prediction error, is then printed to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hqjH9mKZmqvk",
    "outputId": "a81aae30-716d-4f26-c712-38414bf42f0f"
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x_batch, y_batch = batch\n",
    "        x_batch = x_batch.to(device)\n",
    "        outputs = model(x_batch)\n",
    "        predictions.extend(outputs.squeeze().tolist())\n",
    "\n",
    "rmse = np.sqrt(np.mean((scaler.inverse_transform(np.array(predictions).reshape(-1, 1)) - scaler.inverse_transform(y_test.numpy().reshape(-1, 1)))**2))\n",
    "print(f\"Score (RMSE): {rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
