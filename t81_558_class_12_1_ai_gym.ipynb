{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N58_HAIBIeX6"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_deep_learning/blob/main/t81_558_class_12_1_ai_gym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnID4yguIeX7"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 12: Reinforcement Learning**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7l5rd2MIeX8"
   },
   "source": [
    "# Module 12 Video Material\n",
    "\n",
    "* **Part 12.1: Introduction to Introduction to Gymnasium** [[Video]](https://www.youtube.com/watch?v=FvuyrpzvwdI&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_1_reinforcement.ipynb)\n",
    "* Part 12.2: Introduction to Q-Learning [[Video]](https://www.youtube.com/watch?v=VKuqvbG_KAw&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_2_qlearningreinforcement.ipynb)\n",
    "* Part 12.3: Stable Baselines Q-Learning [[Video]](https://www.youtube.com/watch?v=kl7zsCjULN0&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_3_pytorch_reinforce.ipynb)\n",
    "* Part 12.4: Atari Games with Stable Baselines Neural Networks [[Video]](https://www.youtube.com/watch?v=maLA1_d4pzQ&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_4_atari.ipynb)\n",
    "* Part 12.5: Future of Reinforcement Learning [[Video]](https://www.youtube.com/watch?v=-euo5pTjP8E&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_5_rl_future.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UImTzmGTIeX9"
   },
   "source": [
    "# Part 12.1: Introduction to Gymnasium\n",
    "\n",
    "[Gymnasium](https://github.com/Farama-Foundation/Gymnasium) aims to provide an easy-to-setup general-intelligence benchmark with various environments. The goal is to standardize how environments are defined in AI research publications to make published research more easily reproducible. The project claims to provide the user with a simple interface. Gymnasium is a fork of the OpenAI Gym, for which OpenAI ceased support in October 2021. Gymnasium is currently supported by [The Farama Foundation](https://farama.org/).\n",
    "\n",
    "Gymnasium is pip-installed onto your local machine. There are a few significant limitations to be aware of:\n",
    "\n",
    "* Gymnasium Atari only **directly** supports Linux and Macintosh\n",
    "* Gymnasium Atari can be used with Windows; however, it requires a particular [installation procedure](https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30)\n",
    "* Gymnasium can not directly render animated games in Google CoLab.\n",
    "\n",
    "Because Gymnasium requires a graphics display, an embedded video is the only way to display Gymnasium in Google CoLab. The presentation of Gymnasium game animations in Google CoLab is discussed later in this module.\n",
    "\n",
    "## Looking at Gymnasium Environments\n",
    "\n",
    "The centerpiece of Gymnasium is the environment, which defines the \"game\" in which your reinforcement algorithm will compete. An environment does not need to be a game; however, it describes the following game-like features:\n",
    "* **action space**: What actions can we take on the environment at each step/episode to alter the environment.\n",
    "* **observation space**: What is the current state of the portion of the environment that we can observe. Usually, we can see the entire environment.\n",
    "\n",
    "Before we begin to look at Gymnasium, it is essential to understand some of the terminology used by this library.\n",
    "\n",
    "* **Agent** - The machine learning program or model that controls the actions.\n",
    "Step - One round of issuing actions that affect the observation space.\n",
    "* **Episode** - A collection of steps that terminates when the agent fails to meet the environment's objective or the episode reaches the maximum number of allowed steps.\n",
    "* **Render** - Gymnasium can render one frame for display after each episode.\n",
    "* **Reward** - A positive reinforcement that can occur at the end of each episode, after the agent acts.\n",
    "* **Non-deterministic** - For some environments, randomness is a factor in deciding what effects actions have on reward and changes to the observation space.\n",
    "\n",
    "Gymnasium must be installed with the following command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5IJAsJACn5z",
    "outputId": "96dabea5-8172-4ce0-e56e-585fae8f06d0"
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium[accept-rom-license,atari]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ub31e1EnCea7"
   },
   "source": [
    "It is important to note that many Gymnasium environments specify that they are not non-deterministic even though they use random numbers to process actions. Based on the Gymnasium GitHub issue tracker, a non-deterministic property means a deterministic environment behaves randomly. Even when you give the environment a consistent seed value, this behavior is confirmed. The program can use the seed method of an environment to seed the random number generator for the environment.\n",
    "\n",
    "The Gymnasium library allows us to query some of these attributes from environments. I created the following function to query Gymnasium environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cciTuR2MIeX-"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "def query_environment(name):\n",
    "    env = gym.make(name)\n",
    "    spec = gym.spec(name)\n",
    "    print(f\"Action Space: {env.action_space}\")\n",
    "    print(f\"Observation Space: {env.observation_space}\")\n",
    "    print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
    "    print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
    "    print(f\"Reward Range: {env.reward_range}\")\n",
    "    print(f\"Reward Threshold: {spec.reward_threshold}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRXfAdwFDwUm"
   },
   "source": [
    "We will look at the **MountainCar-v0** environment, which challenges an underpowered car to escape the valley between two mountains.  The following code describes the Mountian Car environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XYwy9cjlJjEH",
    "outputId": "3455c71d-cb09-467a-c6ec-1c285c8369ec"
   },
   "outputs": [],
   "source": [
    "query_environment(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TsQiaGJE3UA"
   },
   "source": [
    "This environment allows three distinct actions: accelerate forward, decelerate, or backward. The observation space contains two continuous (floating point) values, as evident by the box object. The observation space is simply the position and velocity of the car. The car has 200 steps to escape for each episode. You would have to look at the code, but the mountain car receives no incremental reward. The only reward for the vehicle occurs when it escapes the valley.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RF4n5cYEMyru",
    "outputId": "5f0e32a9-b9a7-40b2-ce02-c169735c942c"
   },
   "outputs": [],
   "source": [
    "query_environment(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwvVKrNebUHJ"
   },
   "source": [
    "The **CartPole-v1** environment challenges the agent to balance a pole while the agent. The environment has an observation space of 4 continuous numbers:\n",
    "\n",
    "* Cart Position\n",
    "* Cart Velocity\n",
    "* Pole Angle\n",
    "* Pole Velocity At Tip\n",
    "\n",
    "To achieve this goal, the agent can take the following actions:\n",
    "\n",
    "* Push cart to the left\n",
    "* Push cart to the right\n",
    "\n",
    "There is also a continuous variant of the mountain car. This version does not simply have the motor on or off. The action space is a single floating-point number for the continuous cart that specifies how much forward or backward force the cart currently utilizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UAlaMcJmNSY0",
    "outputId": "4d4b1e22-7898-4712-8242-0a5bed7d50fa"
   },
   "outputs": [],
   "source": [
    "query_environment(\"MountainCarContinuous-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liyNcsthtSFp"
   },
   "source": [
    "Gymnasium provides a versatile platform for developing and comparing reinforcement learning algorithms. It supports a wide range of environments, including classic Atari games through the Arcade Learning Environment (ALE) emulator. This integration allows researchers and enthusiasts to access a suite of retro video games originally designed for the Atari 2600 console, using them as benchmarks for AI performance. By interfacing with ALE, Gymnasium users can easily implement their algorithms and test them against the nuanced challenges presented by these games. Each game presents unique scenarios that can help in training algorithms to learn various tasks, making Gymnasium an invaluable tool for advancing the field of artificial intelligence through these interactive and complex environments.\n",
    "\n",
    "Reinforcement learning (RL) algorithms can receive input from an Atari game in two primary ways, which cater to different aspects of the game's state and complexity.\n",
    "\n",
    "The first method involves monitoring the game \"screen\" or the visual output that the game generates. In this approach, the RL algorithm processes the pixels of the game display as its environment's state. This is akin to how a human player would see and interpret the game. The algorithm analyzes the patterns, movements, and changes within the frames to make decisions about the best action to take at each step. This method requires the RL model to handle high-dimensional data and learn to associate visual cues with game outcomes.\n",
    "\n",
    "The second method is by monitoring the Atari system's RAM. Despite its limited capacity, the RAM of an Atari system contains all the information about the game's internal state, such as the location of objects, player scores, and game status. By tapping into this memory directly, an RL algorithm can access a more compact and less noisy representation of the game state than the pixel data provides. This can be beneficial for learning more efficiently as the system's state is represented in a more structured and lower-dimensional form.\n",
    "\n",
    "Both methods have their merits. The screen capture approach forces the algorithm to learn directly from visual input, which is a more general approach and closer to how humans play games. On the other hand, the RAM monitoring method can lead to quicker training times and potentially a deeper understanding of the game mechanics, as it bypasses the need to interpret visual data. Choosing between these methods depends on the specific goals and constraints of the RL project at hand.\n",
    "\n",
    "First, we see how to monitor the screen of the game [Breakout](https://gymnasium.farama.org/environments/atari/breakout/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ndTb-9pgJizW",
    "outputId": "679b47ca-16c6-4c16-ec92-fb1b0c9be2ec"
   },
   "outputs": [],
   "source": [
    "query_environment(\"ALE/Breakout-v5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BzeW4zwuCTH"
   },
   "source": [
    "Similarly, we can monitor the RAM of Breakout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ni1rxzmLKAdH",
    "outputId": "a42239fd-465f-44df-dba0-17f55b7b4528"
   },
   "outputs": [],
   "source": [
    "query_environment(\"ALE/Breakout-ram-v5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3E253PBGPRuw"
   },
   "source": [
    "## Render OpenAI Gym Environments from CoLab\n",
    "\n",
    "It is possible to visualize the game your agent is playing, even on CoLab. This section provides information on generating a video in CoLab that shows you an episode of the game your agent is playing. I based this video process on suggestions found [here](https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t).\n",
    "\n",
    "Begin by installing **pyvirtualdisplay** and **python-opengl**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uF92FCzZMWPn",
    "outputId": "f29ec706-829f-4aa1-f550-3776de2b806c"
   },
   "outputs": [],
   "source": [
    "# HIDE OUTPUT\n",
    "!pip install pyvirtualdisplay\n",
    "!sudo apt-get install -y xvfb ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hS7L8kFMLkjN"
   },
   "source": [
    "Next, we install the needed requirements to display an Atari game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78BfQoQKOq7z",
    "outputId": "45eff938-6586-43db-d2bd-628870130783"
   },
   "outputs": [],
   "source": [
    "# HIDE OUTPUT\n",
    "!sudo apt-get install xvfb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjTHm2SpLz10"
   },
   "source": [
    "Note, the above cell may request to restart the runtime, if this occurs, please restart the CoLab runtime. Next, we define the functions used to show the video by adding it to the CoLab notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6NATj-kNADT"
   },
   "source": [
    "Now we are ready to play the game.  We use a simple random agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "-YNuidnZ6CKv",
    "outputId": "d5782bef-b51d-494d-c608-c84a55d59328"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gymnasium as gymnasium\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Start virtual display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "# Create Atlantis environment\n",
    "env = gymnasium.make('Atlantis-v4', render_mode=\"rgb_array\")\n",
    "env.metadata['render_fps'] = 30\n",
    "# Reset the environment\n",
    "env.reset()\n",
    "\n",
    "# Setup the wrapper to record the video\n",
    "video_callable=lambda episode_id: True\n",
    "env = RecordVideo(env, video_folder='./videos', episode_trigger=video_callable)\n",
    "\n",
    "# Run the environment until done\n",
    "terminated = False\n",
    "truncated = False\n",
    "while not (terminated or truncated):\n",
    "    action = env.action_space.sample()  # replace with your own policy!\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Display the video\n",
    "video = io.open(glob.glob('videos/*.mp4')[0], 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "ipythondisplay.display(HTML(data='''\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "    </video>\n",
    "'''.format(encoded.decode('ascii'))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuxXMsPB8aya"
   },
   "source": [
    "You will note that the **step** and **reset** functions return several values:\n",
    "\n",
    "* **observation** (ObsType): An element of the environment's observation_space as the next observation due to the agent actions. An example is a numpy array containing the positions and velocities of the pole in CartPole.\n",
    "\n",
    "* **reward** (SupportsFloat): The reward as a result of taking the action.\n",
    "\n",
    "* **terminated** (bool): Whether the agent reaches the terminal state (as defined under the MDP of the task) which can be positive or negative. An example is reaching the goal state or moving into the lava from the Sutton and Barton, Gridworld. If true, the user needs to call reset().\n",
    "\n",
    "* **truncated** (bool): Whether the truncation condition outside the scope of the MDP is satisfied. Typically, this is a timelimit, but could also be used to indicate an agent physically going out of bounds. Can be used to end the episode prematurely before a terminal state is reached. If true, the user needs to call reset().\n",
    "\n",
    "* **info** (dict): Contains auxiliary diagnostic information (helpful for debugging, learning, and logging). This might, for instance, contain: metrics that describe the agent's performance state, variables that are hidden from observations, or individual reward terms that are combined to produce the total reward."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
