{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klGNgWREsvQv"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_deep_learning/blob/main/t81_558_class_12_4_atari.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzzbqc-JS2z9"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 12: Reinforcement Learning**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCrXGd_CS6eB"
   },
   "source": [
    "# Module 12 Video Material\n",
    "\n",
    "* Part 12.1: Introduction to Introduction to Gymnasium [[Video]](https://www.youtube.com/watch?v=FvuyrpzvwdI&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_1_reinforcement.ipynb)\n",
    "* Part 12.2: Introduction to Q-Learning [[Video]](https://www.youtube.com/watch?v=VKuqvbG_KAw&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_2_qlearningreinforcement.ipynb)\n",
    "* Part 12.3: Stable Baselines Q-Learning [[Video]](https://www.youtube.com/watch?v=kl7zsCjULN0&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_3_pytorch_reinforce.ipynb)\n",
    "* **Part 12.4: Atari Games with Stable Baselines Neural Networks** [[Video]](https://www.youtube.com/watch?v=maLA1_d4pzQ&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_4_atari.ipynb)\n",
    "* Part 12.5: Future of Reinforcement Learning [[Video]](https://www.youtube.com/watch?v=-euo5pTjP8E&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_5_rl_future.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmDI-h7cI0tI"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running the correct version of TensorFlow, and has the necessary Python libraries installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9KQhYThvTCQC",
    "outputId": "9673a6c4-0366-4d70-9645-f6c25c84b664"
   },
   "outputs": [],
   "source": [
    "# HIDE OUTPUT\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "if COLAB:\n",
    "  !pip install stable-baselines3[extra] gymnasium\n",
    "  !pip install gymnasium[accept-rom-license,atari]\n",
    "  !pip install pyvirtualdisplay\n",
    "  !sudo apt-get install -y python-opengl ffmpeg\n",
    "  !sudo apt-get install -y xvfb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsaQlK8fFQqH"
   },
   "source": [
    "# Part 12.4: Atari Games with Stable Baselines Neural Networks\n",
    "\n",
    "The Atari 2600 is a home video game console from Atari, Inc., Released on September 11, 1977. Most credit the Atari with popularizing microprocessor-based hardware and games stored on ROM cartridges instead of dedicated hardware with games built into the unit. Atari bundled their console with two joystick controllers, a conjoined pair of paddle controllers, and a game cartridge: initially [Combat](https://en.wikipedia.org/wiki/Combat_(Atari_2600)), and later [Pac-Man](https://en.wikipedia.org/wiki/Pac-Man_(Atari_2600)).\n",
    "\n",
    "Atari emulators are popular and allow gamers to play many old Atari video games on modern computers. These emulators are even available as JavaScript.\n",
    "\n",
    "* [Virtual Atari](http://www.virtualatari.org/listP.html)\n",
    "\n",
    "Atari games have become popular benchmarks for AI systems, particularly reinforcement learning. OpenAI Gym internally uses the [Stella Atari Emulator](https://stella-emu.github.io/). You can see the Atari 2600 in Figure 12.ATARI.\n",
    "\n",
    "**Figure 12.ATARI: The Atari 2600**\n",
    "![Atari 2600 Console](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/atari-1.png \"Atari 2600 Console\")\n",
    "\n",
    "## Actual Atari 2600 Specs\n",
    "\n",
    "* CPU: 1.19 MHz MOS Technology 6507\n",
    "* Audio + Video processor: Television Interface Adapter (TIA)\n",
    "* Playfield resolution: 40 x 192 pixels (NTSC). It uses a 20-pixel register that is mirrored or copied, left side to right side, to achieve the width of 40 pixels.\n",
    "* Player sprites: 8 x 192 pixels (NTSC). Player, ball, and missile sprites use pixels 1/4 the width of playfield pixels (unless stretched).\n",
    "* Ball and missile sprites: 1 x 192 pixels (NTSC).\n",
    "* Maximum resolution: 160 x 192 pixels (NTSC). Max resolution is achievable only with programming tricks that combine sprite pixels with playfield pixels.\n",
    "* 128 colors (NTSC). 128 possible on screen. Max of 4 per line: background, playfield, player0 sprite, and player1 sprite. Palette switching between lines is common. Palette switching mid-line is possible but not common due to resource limitations.\n",
    "* 2 channels of 1-bit monaural sound with 4-bit volume control.\n",
    "\n",
    "## Gymnasium Atari Breakout\n",
    "\n",
    "You can use OpenAI Gym with Windows; however, it requires a special [installation procedure](https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30).\n",
    "\n",
    "This chapter demonstrates playing [Atari Breakout](https://en.wikipedia.org/wiki/Breakout_(video_game)). Atari Breakout is a classic arcade game that was released by Atari, Inc. in 1976. In the game, the player controls a paddle at the bottom of the screen, using it to bounce a ball against a wall of bricks at the top. The objective is to destroy all the bricks by hitting them with the ball, which the player deflects with the paddle. As the player progresses, the ball moves increasingly faster, and some bricks may require multiple hits to break. The player loses a turn when the ball misses the paddle and hits the bottom of the screen. The simplicity of Breakout's gameplay, combined with its increasing difficulty as the game progresses, has made it a quintessential example of the easy-to-learn-yet-hard-to-master design ethos that characterized many early video games.\n",
    "\n",
    "In the context of artificial intelligence research and particularly within reinforcement learning, Atari Breakout has been adapted as an environment within OpenAI's Gym toolkit, a collection of environments that provide a standardized interface for algorithm development and benchmarking. Stable Baselines is a set of high-quality implementations of reinforcement learning algorithms, which offers a simple way to train and evaluate agents on various tasks, including playing Atari games like Breakout. The adaptation of Breakout to the Gym environment, often referred to as 'Breakout-v0' or 'BreakoutDeterministic-v4' in the Gym library, abstracts the game's mechanics into observations, actions, and rewards, which an AI agent can interact with. In this setup, the agent observes the game state (typically the pixel data from the screen), selects actions (like moving the paddle left or right), and receives rewards (such as the score for breaking bricks). This allows researchers and enthusiasts to apply and test reinforcement learning algorithms using Stable Baselines to develop AI agents that can learn to play Breakout at a superhuman level, offering a playground to advance the field of machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBc9lj9VWWtZ"
   },
   "source": [
    "## Training the Agent\n",
    "\n",
    "We are now ready to train the DQN. Depending on how many episodes you wish to run through, this process can take many hours. This code will update both the loss and average return as training occurs. As training becomes more successful, the average return should increase. The losses reported reflecting the average loss for individual training batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aq8VWUBFz5_t",
    "outputId": "0ee50424-66a5-4975-b65d-13efe4b5d0ad"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "\n",
    "# Set this constant to either 'Breakout' or 'Atlantis' to choose the game\n",
    "GAME_NAME = 'Breakout'  # Or 'Atlantis'\n",
    "\n",
    "# Create the game environment, note that we wrap it with VecFrameStack for preprocessing\n",
    "env_id = f\"{GAME_NAME}NoFrameskip-v4\"\n",
    "env = make_atari_env(env_id, n_envs=4, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "# Initialize the agent, here we use Proximal Policy Optimization (PPO)\n",
    "model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=\"./atari_ppo_tensorboard/\")\n",
    "\n",
    "# Train the agent\n",
    "TIMESTEPS = 1e5\n",
    "model.learn(total_timesteps=TIMESTEPS)\n",
    "\n",
    "# Save the model\n",
    "model.save(f\"{GAME_NAME}_ppo_model\")\n",
    "\n",
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "# Don't forget to close the environment when you are done\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7-XpPP99Cy7"
   },
   "source": [
    "## Videos\n",
    "\n",
    "Perhaps the most compelling way to view an Atari game's results is a video that allows us to see the agent play the game. We now have a trained model and observed its training progress on a graph. The following functions are defined to watch the agent play the game in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Qpx5qvI3wD0",
    "outputId": "544f2f62-b04a-4a79-d504-cfcc6df7d521"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
    "from stable_baselines3 import PPO\n",
    "import os\n",
    "\n",
    "# Set the game name here\n",
    "GAME_NAME = 'Breakout'  # Can be 'Atlantis' as well\n",
    "\n",
    "# Load your previously trained model\n",
    "model_path = f\"{GAME_NAME}_ppo_model.zip\"\n",
    "model = PPO.load(model_path)\n",
    "\n",
    "# Create the Atari environment and apply the correct wrappers\n",
    "env_id = f\"{GAME_NAME}NoFrameskip-v4\"\n",
    "env = make_atari_env(env_id, n_envs=1, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "# Record the environment\n",
    "video_folder = '/content/videos'\n",
    "if not os.path.exists(video_folder):\n",
    "    os.makedirs(video_folder)\n",
    "\n",
    "env = VecVideoRecorder(env, video_folder,\n",
    "                       record_video_trigger=lambda step: step == 0,\n",
    "                       video_length=500,\n",
    "                       name_prefix=f\"{GAME_NAME}-agent\")\n",
    "\n",
    "# Reset the environment and observe the initial observation shape\n",
    "obs = env.reset()\n",
    "print(\"Initial observation shape:\", obs.shape)  # Should be (1, 4, 84, 84)\n",
    "\n",
    "# Run one episode\n",
    "done = False\n",
    "while not done:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "# Close the environment which should also save the video\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "id": "YTTIWddpAawT",
    "outputId": "45e91a16-e603-472c-abe2-d00e2c0c8a4d"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "# Load the video and encode it\n",
    "video_path = '/content/videos/'  # Make sure this matches the path where the videos are saved\n",
    "video_files = [f for f in os.listdir(video_path) if f.endswith('.mp4')]\n",
    "\n",
    "if video_files:\n",
    "    video_filename = video_files[-1]  # if you expect multiple videos, modify this to select the correct one\n",
    "    full_video_filename = f\"{video_path}/{video_filename}\"\n",
    "    mp4 = open(full_video_filename, 'rb').read()\n",
    "    encoded = b64encode(mp4).decode('ascii')\n",
    "    html = HTML(data=f'<video width=\"640\" height=\"480\" controls><source src=\"data:video/mp4;base64,{encoded}\" type=\"video/mp4\"></video>')\n",
    "else:\n",
    "    html = HTML(data=\"Error: No video found\")\n",
    "\n",
    "html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPENO0Ne74f0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
