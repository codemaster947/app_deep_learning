{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klGNgWREsvQv"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_deep_learning/blob/main/t81_558_class_12_3_pytorch_reinforce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmDI-h7cI0tI"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 12: Reinforcement Learning**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsaQlK8fFQqH"
   },
   "source": [
    "# Module 12 Video Material\n",
    "\n",
    "* Part 12.1: Introduction to Introduction to Gymnasium [[Video]](https://www.youtube.com/watch?v=FvuyrpzvwdI&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_1_reinforcement.ipynb)\n",
    "* Part 12.2: Introduction to Q-Learning [[Video]](https://www.youtube.com/watch?v=VKuqvbG_KAw&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_2_qlearningreinforcement.ipynb)\n",
    "* **Part 12.3: Stable Baselines Q-Learning** [[Video]](https://www.youtube.com/watch?v=kl7zsCjULN0&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_3_pytorch_reinforce.ipynb)\n",
    "* Part 12.4: Atari Games with Stable Baselines Neural Networks [[Video]](https://www.youtube.com/watch?v=maLA1_d4pzQ&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_4_atari.ipynb)\n",
    "* Part 12.5: Future of Reinforcement Learning [[Video]](https://www.youtube.com/watch?v=-euo5pTjP8E&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_5_rl_future.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKOCZlhUgXVK"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running the correct version of TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y0WC-M08zNNq",
    "outputId": "c17cd0dd-acb7-46d2-cea5-20fe3bf31a46"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1u9QVVsShC9X"
   },
   "source": [
    "# Part 12.3: Stable-Baselines Q-Learning in Gymnasium\n",
    "\n",
    "As we covered in the previous part, Q-Learning is a robust machine learning algorithm. Unfortunately, Q-Learning requires that the Q-table contain an entry for every possible state that the environment can take. Traditional Q-learning might be a good learning algorithm if the environment only includes a handful of discrete state elements. However, the Q-table can become prohibitively large if the state space is large.\n",
    "\n",
    "Creating policies for large state spaces is a task that Deep Q-Learning Networks (DQN) can usually handle. Neural networks can generalize these states and learn commonalities. Unlike a table, a neural network does not require the program to represent every combination of state and action. A DQN maps the state to its input neurons and the action Q-values to the output neurons. The DQN effectively becomes a function that accepts the state and suggests action by returning the expected reward for each possible action. Figure 12.DQL demonstrates the DQN structure and mapping between state and action.\n",
    "\n",
    "**Figure 12.DQL: Deep Q-Learning (DQL)**\n",
    "![Deep Q-Learning](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/deepqlearning.png \"Reinforcement Learning\")\n",
    "\n",
    "As this diagram illustrates, the environment state contains several elements. For the basic DQN, the state can be a mix of continuous and categorical/discrete values. For the DQN, the discrete state elements the program typically encoded as dummy variables. The actions should be discrete when your program implements a DQN. Other algorithms support continuous outputs, which we will discuss later in this chapter.\n",
    "\n",
    "In the landscape of deep learning, the [Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/) library emerges as a torchbearer for reinforcement learning (RL) enthusiasts and researchers opting for PyTorch. As an evolution of the original Stable Baselines framework, this iteration has been meticulously reforged with the PyTorch backend, providing a suite of reliable, high-performance RL algorithms. It is designed for ease of use, offering a straightforward API that invites both novices and seasoned practitioners to implement, experiment with, and extend upon cutting-edge RL methods. With Stable Baselines 3, one can expect robust pre-trained models, customizable neural network architectures, and comprehensive documentation that empowers users to deploy RL solutions efficiently. Its compatibility with PyTorch means that it seamlessly integrates into the dynamic ecosystem of deep learning tools, allowing for rapid prototyping and research iteration. Whether the goal is to solve discrete control tasks or navigate the complexities of high-dimensional environments.\n",
    "\n",
    "## DQN and the Cart-Pole Problem\n",
    "\n",
    "Barto (1983) first described the cart-pole problem. [[Cite:barto1983neuronlike]](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf) A cart is connected to a rigid hinged pole. The cart is free to move only in the vertical plane of the cart/track. The agent can apply an impulsive \"left\" or \"right\" force F of a fixed magnitude to the cart at discrete time intervals. The cart-pole environment simulates the physics behind keeping the pole reasonably upright position on the cart. The environment has four state variables:\n",
    "* $x$ The position of the cart on the track.\n",
    "* $\\theta$ The angle of the pole with the vertical\n",
    "* $\\dot{x}$ The cart velocity.\n",
    "* $\\dot{\\theta}$ The rate of change of the angle.\n",
    "\n",
    "The action space consists of discrete actions:\n",
    "* Apply force left\n",
    "* Apply force right\n",
    "\n",
    "First, we must install Stable Baselines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FreS1XOPzO9X",
    "outputId": "7fcf5f94-424f-42c6-aa49-452f7489de59"
   },
   "outputs": [],
   "source": [
    "# HIDE OUTPUT\n",
    "if COLAB:\n",
    "  !pip install stable-baselines3[extra] gymnasium\n",
    "  !pip install gymnasium[accept-rom-license,atari]\n",
    "  !pip install pyvirtualdisplay\n",
    "  !sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJCgJnx3g0yY"
   },
   "source": [
    "### The Cartpole Environment\n",
    "In the Cartpole environment:\n",
    "\n",
    "-   `observation` is an array of 4 floats:\n",
    "    -   the position and velocity of the cart\n",
    "    -   the angular position and velocity of the pole\n",
    "-   `reward` is a scalar float value\n",
    "-   `action` is a scalar integer with only two possible values:\n",
    "    -   `0` — \"move left\"\n",
    "    -   `1` — \"move right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2UGR5t_iZX-",
    "outputId": "925bbefb-e329-4d9e-f287-c336282176a9"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Create and initialize the MountainCar environment\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "time_step = env.reset()\n",
    "print('Time step:')\n",
    "print(time_step)\n",
    "\n",
    "action = 1\n",
    "\n",
    "next_time_step = env.step(action)\n",
    "print('Next time step:')\n",
    "print(next_time_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuUqXAVmecTU"
   },
   "source": [
    "We can also visualize this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "id": "JpgsdYthvwTo",
    "outputId": "bd78d5b9-1bcf-461f-c2bd-306145b74cc6"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Render the environment's state to a numpy array\n",
    "frame = env.render()\n",
    "\n",
    "# Convert the numpy array to an image and display it\n",
    "image = Image.fromarray(frame)\n",
    "\n",
    "# Don't forget to close the environment when you're done!\n",
    "env.close()\n",
    "\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_snCVvq5Z8lJ"
   },
   "source": [
    "The goal is to move the above cart without causing the pole to fall over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBc9lj9VWWtZ"
   },
   "source": [
    "## Training the agent\n",
    "\n",
    "We will make use of Stable-Baselines3 to train an agent for this environment. We will make use of the MlpPolicy, which makes use of a Multi-Layer Peceptron (MLP), which is another name for neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GBq91RRQf5Ha",
    "outputId": "abd174e5-3b67-4f36-a14a-8564041cbe80"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Create the CartPole environment\n",
    "env = make_vec_env('CartPole-v1', n_envs=1)\n",
    "\n",
    "# Instantiate the agent\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the agent\n",
    "model.save(\"ppo_cartpole\")\n",
    "\n",
    "# Create a fresh environment for evaluation\n",
    "eval_env = gym.make('CartPole-v1')\n",
    "\n",
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7-XpPP99Cy7"
   },
   "source": [
    "## Videos\n",
    "\n",
    "We can easily visulaize the cart pole ageint in a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 584
    },
    "id": "9Q9DHuLyf_B1",
    "outputId": "c2dde222-38cd-4caf-8f53-c2f35ae63af1"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "import base64\n",
    "from IPython import display as ipythondisplay\n",
    "from pathlib import Path\n",
    "\n",
    "# Record the agent playing\n",
    "video_folder = '/videos'\n",
    "video_length = 1500\n",
    "\n",
    "env = make_vec_env('CartPole-v1', n_envs=1)\n",
    "env = VecVideoRecorder(env, video_folder,\n",
    "                       record_video_trigger=lambda x: x == 0, video_length=video_length,\n",
    "                       name_prefix=\"ppo-cartpole\")\n",
    "\n",
    "obs = env.reset()\n",
    "for _ in range(video_length):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, _, _, _ = env.step(action)\n",
    "\n",
    "# Close the environment and video recorder\n",
    "env.close()\n",
    "\n",
    "# Display the video\n",
    "video_path = Path(video_folder) / \"ppo-cartpole-step-0-to-step-1500.mp4\"\n",
    "video = open(video_path, \"rb\").read()\n",
    "encoded = base64.b64encode(video)\n",
    "\n",
    "ipythondisplay.display(ipythondisplay.HTML(data=f'<video alt=\"test\" autoplay loop controls style=\"height: 400px;\">'\n",
    "                                        f'<source src=\"data:video/mp4;base64,{encoded.decode()}\" type=\"video/mp4\" />'\n",
    "                                        f'</video>'))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
