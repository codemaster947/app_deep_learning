{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZggjUZ5oPvzH"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_deep_learning/blob/main/t81_558_class_09_2_face_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDTXd8-Lmp8Q"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "\n",
    "**Module 9: Facial Recognition**\n",
    "\n",
    "- Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "- For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncNrAEpzmp8S"
   },
   "source": [
    "# Module 9 Material\n",
    "\n",
    "- Part 9.1: Detecting Faces in an Image [[Video]](https://www.youtube.com/watch?v=Hpp3D3P2iWQ&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_09_1_faces.ipynb)\n",
    "- **Part 9.2: Detecting Facial Features** [[Video]](https://www.youtube.com/watch?v=AblTbq0T2wE&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_09_2_face_features.ipynb)\n",
    "- Part 9.3: Reality Augmentation [[Video]](https://www.youtube.com/watch?v=jfZDiRxx5Bc&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_09_3_reality_augmentation.ipynb)\n",
    "- Part 9.4: Application: Emotion Detection [[Video]](https://www.youtube.com/watch?v=F0H6vojQhE8&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_09_4_emotion.ipynb)\n",
    "- Part 9.5: Application: Blink Efficiency [[Video]](https://www.youtube.com/watch?v=96LPEStHCUA&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_09_5_blink.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKQqQljyPvzK"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code checks that Google CoLab is and sets up the correct hardware settings for PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fU9UhAxTmp8S",
    "outputId": "29c1a863-a45f-4ff6-d5d8-323939fcfcb9"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# Make use of a GPU or MPS (Apple) if one is available.  (see module 3.2)\n",
    "import torch\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "if device!='cuda':\n",
    "  print(\"*******WARNING, this notebook requires a CUDA GPU****\")\n",
    "  print(\"This notebook will not work correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-qb-mcqmp8U"
   },
   "source": [
    "# Part 9.2: Detecting Facial Features\n",
    "\n",
    "Facial landmark detection is a sophisticated aspect of computer vision that focuses on identifying and locating specific points of interest on a face. These points, or landmarks, typically correspond to anatomically meaningful positions such as the corners of the eyes, the tip of the nose, the endpoints of the mouth, and the contour of the jawline, among others. The aim is to capture the geometric structure and features of a face in a detailed manner. Once detected, these landmarks serve as pivotal reference points for a plethora of applications ranging from facial alignment and emotion recognition to augmented reality filters and advanced facial animation. The precision and reliability of facial landmark detection algorithms are paramount, given their profound implications in fields like security, healthcare, entertainment, and user experience enhancement. Detecting these landmarks allows for a deeper understanding and interpretation of facial expressions, movements, and nuances, bridging the gap between mere face detection and intricate facial analysis.\n",
    "\n",
    "The detection of these landmarks opens the door to a myriad of applications:\n",
    "\n",
    "* **Facial Alignment**: Landmarks can be used to align faces, ensuring that the eyes, nose, and mouth are consistently positioned. This is crucial for facial recognition systems as it can help in normalizing face images for comparison.\n",
    "\n",
    "* **Emotion Recognition**: By analyzing the position and movement of facial landmarks, especially around the mouth and eyebrows, it's possible to deduce a person's emotional state.\n",
    "\n",
    "* **Face Morphing and Augmentation**: Landmarks serve as reference points for morphing one face into another or for augmenting features in applications like Snapchat or Instagram filters.\n",
    "\n",
    "* **Face Animation**: In animation and gaming, facial landmarks can be used to map and replicate human facial expressions onto animated characters.\n",
    "\n",
    "* **Gaze Tracking**: By observing the position and orientation of eye landmarks, it's possible to determine where a person might be looking.\n",
    "\n",
    "* **Facial Feature Extraction**: For cosmetic or medical applications, precise landmarking can assist in feature extraction, measuring facial symmetry, or planning surgeries.\n",
    "\n",
    "* **Speech Therapy and Lip Reading**: Landmarks around the mouth can assist in understanding lip movements, aiding in speech therapy or in developing lip-reading algorithms.\n",
    "\n",
    "Currently one of the most accurate facial landmark detection algorithms is [Shape Preserving Facial Landmarks with Graph Attention Networks (SPIGA)](https://github.com/andresprados/SPIGA). The authors of this paper also provided working code at a [GitHub](https://github.com/andresprados/SPIGA) repository. In this section we will make use of facenet-pytorch to detect faces and SPIGA to find 97 different facial landmarks on individual faces.\n",
    "\n",
    "We begin by installing SPIGA and facenet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "53sLzbmvxYSB",
    "outputId": "b3143863-252a-443c-9409-9304d285e8ca"
   },
   "outputs": [],
   "source": [
    "# Setup SPIGA and facenet\n",
    "!pip install facenet-pytorch\n",
    "!git clone https://github.com/andresprados/SPIGA.git\n",
    "%cd SPIGA/\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5eH1LBfAKlw"
   },
   "source": [
    "Previously, we worked with images in the PIL format in Python. OpenCV is another standard image processing library in Python, often explicitly used by machine learning applications. In this chapter, we will need to use both PIL and OpenCV. The MTCNN model we use to detect faces accepts PIL images, whereas the SPIGA model uses OpenCV-style images to detect facial landmarks. This requirement means that our code will need to support both formats. The following code loads an image both as OpenCV and PIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JNuIn5cxeH6"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "url = \"https://data.heatonresearch.com/images/jeff/about-jeff-heaton-2020.jpg\"\n",
    "\n",
    "# Download the image using requests\n",
    "response = requests.get(url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "response.raise_for_status()\n",
    "\n",
    "# Convert the downloaded bytes to a numpy array\n",
    "image = np.asarray(bytearray(response.content), dtype=\"uint8\")\n",
    "\n",
    "# Decode the numpy array to an OpenCV image\n",
    "img = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "\n",
    "# Convert the OpenCV image (NumPy array) to a PIL Image\n",
    "image_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "image_pil = Image.fromarray(image_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "US0Plxh4D_NW"
   },
   "source": [
    "Next we detect the bounding boxes for all images. The first bounding box has the highest probability of being a face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjlTheR3BA0Q"
   },
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "\n",
    "# Detect faces\n",
    "boxes, _ = mtcnn.detect(image_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmZ0nyqqEOiE"
   },
   "source": [
    "The following code uses PIL to draw rectangles around all detected faces. Though this image only contains one face, multiple false positives are detected. If you can assume that there will be only one face in an image, it is safe to simply use the first rectangle as the face, which is the approach we will take in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "Jjq2rvOG1qoA",
    "outputId": "994ed2f1-a25d-40ed-cd44-65dc7337a669"
   },
   "outputs": [],
   "source": [
    "# Prepare to draw on the image\n",
    "draw = ImageDraw.Draw(image_pil)\n",
    "\n",
    "# Draw bounding boxes\n",
    "for box in boxes:\n",
    "    draw.rectangle(box.tolist(), outline=\"red\", width=12)\n",
    "    print(box)\n",
    "    print(f\"Found a face at the bounding box, Left: {box[0]}, Top: {box[1]}, Right: {box[2]}, Bottom: {box[3]}\")\n",
    "\n",
    "# Scale and show image results\n",
    "aspect_ratio = image_pil.height / image_pil.width\n",
    "new_height = int(aspect_ratio * 512)\n",
    "\n",
    "# Resize the image\n",
    "display(image_pil.resize((512, new_height)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWF03pBJVCEn"
   },
   "source": [
    "Next we will call SPIGA and provide the bounding box for the detected face. SPIGA will return a list of 97 detected facial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v3Dz9cg2xkHt",
    "outputId": "081c63f5-697c-4fce-de11-3d60bad28a77"
   },
   "outputs": [],
   "source": [
    "from spiga.inference.config import ModelConfig\n",
    "from spiga.inference.framework import SPIGAFramework\n",
    "\n",
    "x0 = int(boxes[0][0])\n",
    "y0 = int(boxes[0][1])\n",
    "x1 = int(boxes[0][2])\n",
    "y1 = int(boxes[0][3])\n",
    "bbox = [x0,y0,y1,y1]\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread(url)\n",
    "\n",
    "print(bbox)\n",
    "#bbox = [165, 100, 338, 338]\n",
    "#bbox = [50,0,100,100]\n",
    "\n",
    "# Process image\n",
    "dataset = 'wflw'\n",
    "processor = SPIGAFramework(ModelConfig(dataset))\n",
    "features = processor.inference(img, [bbox])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nr2tGGHXDhM"
   },
   "source": [
    "The provided code is designed to visualize specific features on an image, most related to SPIGA's facial landmark recognition. First, the code imports the necessary modules, including  **Plotter** from SPIGA, which is specificially designed to display SPIGA's output. The image is displayed by **cv2_imshow**, which is specifically made for displaying images in Google Colab. After preparing the required variables, a deep copy of the image (img) is stored in the variable canvas. It then extracts landmark and head pose information from the features dictionary. With the use of the **Plotter** object, these landmarks and the head pose are drawn onto the canvas. In addition to this, a green-colored bounding box is also plotted on the canvas around coordinates (x0, y0) and (x1, y1). Finally, the modified image is resized to a width of 512 pixels while maintaining its aspect ratio, and then it's displayed within the Google Colab environment using cv2_imshow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "TvlDJu5Rxoll",
    "outputId": "968b0e1d-5fcf-44bd-d4d8-a8e29f4d0ecd"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from spiga.demo.visualize.plotter import Plotter\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# Prepare variables\n",
    "\n",
    "\n",
    "canvas = copy.deepcopy(img)\n",
    "landmarks = np.array(features['landmarks'][0])\n",
    "headpose = np.array(features['headpose'][0])\n",
    "\n",
    "# Plot features\n",
    "plotter = Plotter()\n",
    "canvas = plotter.landmarks.draw_landmarks(canvas, landmarks)\n",
    "canvas = plotter.hpose.draw_headpose(canvas, [x0, y0, x1, y1], headpose[:3], headpose[3:], euler=True)\n",
    "\n",
    "# Bounding box\n",
    "color = (0, 255, 0)  # Green color\n",
    "thickness = 2\n",
    "cv2.rectangle(canvas, (x0,y0), (x1,y1), color, thickness)\n",
    "\n",
    "\n",
    "# Show image results\n",
    "(h, w) = canvas.shape[:2]\n",
    "canvas = cv2.resize(canvas, (512, int(h*512/w)))\n",
    "cv2_imshow(canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lRenN1vYrKU"
   },
   "source": [
    "The facial landmarks are numbered, as seen below. You can make use of these array indexes to find the facial data you need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPfGiJ0cR01q"
   },
   "source": [
    "![Facial Landmarks](https://data.heatonresearch.com/images/wustl/class/face-landmarks.jpg)\n",
    "\n",
    "* Facial Perimeter: points 0 to 32 (green)\n",
    "* Right Eyebrow: points 33 to 41 (blue)\n",
    "* Left Eyebrow: points 42 to 50 (blue)\n",
    "* Nose: points 51 to 59 (purple)\n",
    "* Right Eye: points 60 to 67 (red)\n",
    "* Left Eye: points 68 to 75 (red)\n",
    "* Lips: points 76 to 87 (cyan)\n",
    "* Mouth: points 88 to 95 (yellow)\n",
    "* Pupals: points 96 (right) and 97 (left, red)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
