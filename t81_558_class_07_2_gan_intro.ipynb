{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oj6p0QRN4JOD"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_deep_learning/blob/main/t81_558_class_07_2_gan_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UL-Y_bXE4JOD"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 7: Image Generative Models**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2m8cZup4JOE"
   },
   "source": [
    "# Module 7 Material\n",
    "\n",
    "- Part 7.1 Introduction to Generative AI [[Video]](https://www.youtube.com/watch?v=2FbkbSnS8sg&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_07_1_img_generative.ipynb)\n",
    "- **Part 7.2 Generating Faces with StyleGAN3** [[Video]](https://www.youtube.com/watch?v=VcI2o1yEQa0&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_07_2_gan_intro.ipynb)\n",
    "- Part 7.3 GANS to Enhance Old Photographs Deoldify [[Video]](https://www.youtube.com/watch?v=y7HvjfKsZ50&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_07_3_deoldify.ipynb)\n",
    "- Part 7.4 Text to Images with StableDiffusion [[Video]](https://www.youtube.com/watch?v=gLj6-gJ-lR4&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_07_4_stable_diff.ipynb)\n",
    "- Part 7.5 Finetuning with Dreambooth [[Video]](https://www.youtube.com/watch?v=G_FYFSzkB5Y&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_07_5_dream_booth.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOcBZtt84rrd"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vrshAzVK4smS",
    "outputId": "a29e0377-5039-479c-d395-a21ddcfbb4f1"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avFNBe3j4JOE"
   },
   "source": [
    "# Part 7.2: Introduction to GANS for Image and Data Generation\n",
    "\n",
    "A generative adversarial network (GAN) is a class of machine learning systems invented by Ian Goodfellow in 2014. [[Cite:goodfellow2014generative]](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) Two neural networks compete with each other in a game. The GAN training algorithm starts with a training set and learns to generate new data with the same distributions as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics.\n",
    "\n",
    "Running this notebook in this notebook in Google CoLab is the most straightforward means of completing this chapter. Because of this, I designed this notebook to run in Google CoLab. It will take some modifications if you wish to run it locally.\n",
    "\n",
    "This original StyleGAN paper used neural networks to automatically generate images for several previously seen datasets: MINST and CIFAR. However, it also included the Toronto Face Dataset (a private dataset used by some researchers). You can see some of these images in Figure 7.GANS.\n",
    "\n",
    "**Figure 7.GANS: GAN Generated Images**\n",
    "![GAN](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan-2.png \"GAN Generated Images\")\n",
    "\n",
    "Only sub-figure D made use of convolutional neural networks. Figures A-C make use of fully connected neural networks. As we will see in this module, the researchers significantly increased the role of convolutional neural networks for GANs.\n",
    "\n",
    "We call a GAN a generative model because it generates new data. You can see the overall process in Figure 7.GAN-FLOW.\n",
    "\n",
    "**Figure 7.GAN-FLOW: GAN Structure**\n",
    "![GAN Structure](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan-1.png \"GAN Structure\")\n",
    "\n",
    "## Face Generation with StyleGAN and Python\n",
    "\n",
    "GANs have appeared frequently in the media, showcasing their ability to generate highly photorealistic faces. One significant step forward for realistic face generation was the NVIDIA StyleGAN series. NVIDIA introduced the origional StyleGAN in 2018. [[Cite:karras2019style]](https://arxiv.org/abs/1812.04948) StyleGAN was followed by StyleGAN2 in 2019, which improved the quality of StyleGAN by removing certian artifacts. [[Cite:karras2019analyzing]](https://arxiv.org/abs/1912.04958) Most recently, in 2020, NVIDIA released StyleGAN2 adaptive discriminator augmentation (ADA), which will be the focus of this module. [[Cite:karras2020training]](https://arxiv.org/abs/2006.06676)  We will see both how to train StyleGAN2 ADA on any arbitray set of images; as well as use pretrained weights provided by NVIDIA. The NVIDIA weights allow us to generate high resolution photorealistic looking faces, such seen in Figure 7.STY-GAN.\n",
    "\n",
    "**Figure 7.STY-GAN: StyleGAN2 Generated Faces**\n",
    "![StyleGAN2 Generated Faces](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/stylegan2_images.jpg \"StyleGAN2 Generated Faces\")\n",
    "\n",
    "The above images were generated with StyleGAN2, using Google CoLab. Following the instructions in this section, you will be able to create faces like this of your own. StyleGAN2 images are usually 1,024 x 1,024 in resolution.  An example of a full-resolution StyleGAN image can be [found here](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/stylegan2-hires.jpg).\n",
    "\n",
    "The primary advancement introduced by the adaptive discriminator augmentation is that the algorithm augments the training images in real-time. Image augmentation is a common technique in many convolution neural network applications. Augmentation has the effect of increasing the size of the training set. Where StyleGAN2 previously required over 30K images for an effective to develop an effective neural network; now much fewer are needed. I used 2K images to train the fish generating GAN for this section. Figure 7.STY-GAN-ADA demonstrates the ADA process.\n",
    "\n",
    "**Figure 7.STY-GAN-ADA: StyleGAN2 ADA Training**\n",
    "![StyleGAN2 Generated Faces](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/stylegan2-ada-teaser-1024x252.jpg \"StyleGAN2 Generated Faces\")\n",
    "\n",
    "The figure shows the increasing probability of augmentation as $p$ increases. For small image sets, the discriminator will generally memorize the image set unless the training algorithm makes use of augmentation. Once this memorization occurs, the discriminator is no longer providing useful information to the training of the generator.\n",
    "\n",
    "While the above images look much more realistic than images generated earlier in this course, they are not perfect. Look at Figure 7.STYLEGAN2. There are usually several tell-tail signs that you are looking at a computer-generated image. One of the most obvious is usually the surreal, dream-like backgrounds. The background does not look obviously fake at first glance; however, upon closer inspection, you usually can't quite discern what a GAN-generated background is. Also, look at the image character's left eye. It is slightly unrealistic looking, especially near the eyelashes.\n",
    "\n",
    "Look at the following GAN face. Can you spot any imperfections?\n",
    "\n",
    "**Figure 7.STYLEGAN2: StyleGAN2 Face**\n",
    "![StyleGAN2 Face](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan_bad.jpg \"StyleGAN2 Face\")\n",
    "\n",
    "* Image A demonstrates the abstract backgrounds usually associated with a GAN-generated image.\n",
    "* Image B exhibits issues that earrings often present for GANs. GANs sometimes have problems with symmetry, particularly earrings.\n",
    "* Image C contains an abstract background and a highly distorted secondary image.\n",
    "* Image D also contains a highly distorted secondary image that might be a hand.\n",
    "\n",
    "Several websites allow you to generate GANs of your own without any software.\n",
    "\n",
    "* [This Person Does not Exist](https://www.thispersondoesnotexist.com/)\n",
    "* [Which Face is Real](http://www.whichfaceisreal.com/)\n",
    "\n",
    "The first site generates high-resolution images of human faces. The second site presents a quiz to see if you can detect the difference between a real and fake human face image.\n",
    "\n",
    "In this chapter, you will learn to create your own StyleGAN pictures using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rq3dZOg_5GNH"
   },
   "source": [
    "## Generating High Rez GAN Faces with Google CoLab\n",
    "\n",
    "This notebook demonstrates how to run [NVidia StyleGAN2 ADA](https://github.com/NVlabs/stylegan2-ada) inside a Google CoLab notebook.  I suggest you use this to generate GAN faces from a pretrained model.  If you try to train your own, you will run into compute limitations of Google CoLab. Make sure to run this code on a GPU instance.  GPU is assumed.\n",
    "\n",
    "First, we clone StyleGAN3 from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qjXaut8E4JOE",
    "outputId": "a7416a54-43e3-4389-91aa-73b450f1dd58"
   },
   "outputs": [],
   "source": [
    "# HIDE OUTPUT\n",
    "!git clone https://github.com/NVlabs/stylegan3.git\n",
    "!pip install ninja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAWeiW2A5Wub"
   },
   "source": [
    "\n",
    "Verify that StyleGAN has been cloned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kZF5jtAT4JOF",
    "outputId": "d5a8dc11-bff3-4e4d-f261-786c4e419ba0"
   },
   "outputs": [],
   "source": [
    "!ls /content/stylegan3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17md6jij5bPv"
   },
   "source": [
    "## Run StyleGan From Command Line\n",
    "\n",
    "Add the StyleGAN folder to Python so that you can import it. I based this code below on code from NVidia for the original StyleGAN paper. When you use StyleGAN you will generally create a GAN from a seed number. This seed is an integer, such as 6600, that will generate a unique image. The seed generates a latent vector containing 512 floating-point values. The GAN code uses the seed to generate these 512 values. The seed value is easier to represent in code than a 512 value vector; however, while a small change to the latent vector results in a slight change to the image, even a small change to the integer seed value will produce a radically different image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UUA_V0yH5f4Y",
    "outputId": "aa70f14b-7d34-4806-f525-44de9f64b786"
   },
   "outputs": [],
   "source": [
    "# HIDE OUTPUT\n",
    "URL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/\"\\\n",
    "      \"stylegan3/versions/1/files/stylegan3-r-ffhq-1024x1024.pkl\"\n",
    "\n",
    "!python /content/stylegan3/gen_images.py \\\n",
    "    --network={URL} \\\n",
    "  --outdir=/content/results --seeds=6600-6625"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5eLRW1mGN-G"
   },
   "source": [
    "We can now display the images created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PVRTbk5S5mPI",
    "outputId": "a2dd1e93-7f88-44b6-d59e-4aac533bc018"
   },
   "outputs": [],
   "source": [
    "!ls /content/results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OPbrrs85jXO"
   },
   "source": [
    "Next, copy the images to a folder of your choice on GDrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLBt0hmv5poU"
   },
   "outputs": [],
   "source": [
    "!cp /content/results/* \\\n",
    "    /content/drive/My\\ Drive/projects/stylegan3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rGCxzWG5uBi"
   },
   "source": [
    "## Run StyleGAN From Python Code\n",
    "\n",
    "Add the StyleGAN folder to Python so that you can import it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDscPcmE5zbs"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/content/stylegan3\")\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display\n",
    "import torch\n",
    "import dnnlib\n",
    "import legacy\n",
    "\n",
    "def seed2vec(G, seed):\n",
    "  return np.random.RandomState(seed).randn(1, G.z_dim)\n",
    "\n",
    "def display_image(image):\n",
    "  plt.axis('off')\n",
    "  plt.imshow(image)\n",
    "  plt.show()\n",
    "\n",
    "def generate_image(G, z, truncation_psi):\n",
    "    # Render images for dlatents initialized from random seeds.\n",
    "    Gs_kwargs = {\n",
    "        'output_transform': dict(func=tflib.convert_images_to_uint8,\n",
    "         nchw_to_nhwc=True),\n",
    "        'randomize_noise': False\n",
    "    }\n",
    "    if truncation_psi is not None:\n",
    "        Gs_kwargs['truncation_psi'] = truncation_psi\n",
    "\n",
    "    label = np.zeros([1] + G.input_shapes[1][1:])\n",
    "    # [minibatch, height, width, channel]\n",
    "    images = G.run(z, label, **G_kwargs)\n",
    "    return images[0]\n",
    "\n",
    "def get_label(G, device, class_idx):\n",
    "  label = torch.zeros([1, G.c_dim], device=device)\n",
    "  if G.c_dim != 0:\n",
    "      if class_idx is None:\n",
    "          ctx.fail(\"Must specify class label with --class when using \"\\\n",
    "            \"a conditional network\")\n",
    "      label[:, class_idx] = 1\n",
    "  else:\n",
    "      if class_idx is not None:\n",
    "          print (\"warn: --class=lbl ignored when running on \"\\\n",
    "            \"an unconditional network\")\n",
    "  return label\n",
    "\n",
    "def generate_image(device, G, z, truncation_psi=1.0, noise_mode='const',\n",
    "                   class_idx=None):\n",
    "  z = torch.from_numpy(z).to(device)\n",
    "  label = get_label(G, device, class_idx)\n",
    "  img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n",
    "  img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(\\\n",
    "      torch.uint8)\n",
    "  return PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWPdx2PX5567",
    "outputId": "552b389b-ea3d-4e0c-c71e-47c93c69dbe1"
   },
   "outputs": [],
   "source": [
    "#URL = \"https://github.com/jeffheaton/pretrained-gan-fish/releases/\"\\\n",
    "#  \"download/1.0.0/fish-gan-2020-12-09.pkl\"\n",
    "#URL = \"https://github.com/jeffheaton/pretrained-merry-gan-mas/releases/\"\\\n",
    "#  \"download/v1/christmas-gan-2020-12-03.pkl\"\n",
    "URL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/\"\\\n",
    "  \"versions/1/files/stylegan3-r-ffhq-1024x1024.pkl\"\n",
    "\n",
    "print(f'Loading networks from \"{URL}\"...')\n",
    "device = torch.device('cuda')\n",
    "with dnnlib.util.open_url(URL) as f:\n",
    "    G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ys0HNy590V33"
   },
   "source": [
    "We can now generate images from integer seed codes in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4QVzyLIj582S",
    "outputId": "5b44ea41-6e37-43fa-f2ce-8913de1fafa5"
   },
   "outputs": [],
   "source": [
    "# Choose your own starting and ending seed.\n",
    "SEED_FROM = 1000\n",
    "SEED_TO = 1003\n",
    "\n",
    "# Generate the images for the seeds.\n",
    "for i in range(SEED_FROM, SEED_TO):\n",
    "  print(f\"Seed {i}\")\n",
    "  z = seed2vec(G, i)\n",
    "  img = generate_image(device, G, z)\n",
    "  display_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N3Dcd2i52UT"
   },
   "source": [
    "## Examining the Latent Vector\n",
    "\n",
    "Figure 7.LVEC shows the effects of transforming the latent vector between two images. We accomplish this transformation by slowly moving one 512-value latent vector to another 512 vector. A high-dimension point between two latent vectors will appear similar to both of the two endpoint latent vectors. Images that have similar latent vectors will appear similar to each other.\n",
    "\n",
    "**Figure 7.LVEC: Transforming the Latent Vector**\n",
    "![GAN](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan_progression.jpg \"GAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "keozDTI_6EcA",
    "outputId": "28bebf24-9d74-43ca-841c-f5d5cfce5443"
   },
   "outputs": [],
   "source": [
    "def expand_seed(seeds, vector_size):\n",
    "  result = []\n",
    "\n",
    "  for seed in seeds:\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    result.append( rnd.randn(1, vector_size) )\n",
    "  return result\n",
    "\n",
    "#URL = \"https://github.com/jeffheaton/pretrained-gan-fish/releases/\"\\\n",
    "#  \"download/1.0.0/fish-gan-2020-12-09.pkl\"\n",
    "#URL = \"https://github.com/jeffheaton/pretrained-merry-gan-mas/releases/\"\\\n",
    "#  \"download/v1/christmas-gan-2020-12-03.pkl\"\n",
    "#URL = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl\"\n",
    "URL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/\"\\\n",
    "  \"versions/1/files/stylegan3-r-ffhq-1024x1024.pkl\"\n",
    "\n",
    "print(f'Loading networks from \"{URL}\"...')\n",
    "device = torch.device('cuda')\n",
    "with dnnlib.util.open_url(URL) as f:\n",
    "    G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n",
    "\n",
    "vector_size = G.z_dim\n",
    "# range(8192,8300)\n",
    "seeds = expand_seed( [8192+1,8192+9], vector_size)\n",
    "#generate_images(Gs, seeds,truncation_psi=0.5)\n",
    "print(seeds[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fCn7OIM6caj"
   },
   "source": [
    "The following code will move between the provided seeds.  The constant STEPS specify how many frames there should be between each seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583,
     "referenced_widgets": [
      "5e3563dfdf904c58bb4d1a87d3b8dee6",
      "b0944d834df24011b630396c77205bb3",
      "1472187917574cd5887aa6bc4cc61b59",
      "27497c7cccbf4851b8a0abb80558ee05",
      "f8a42fd6b95948d0998a1c5261bdb14a",
      "99235a067d32492086daebc98ee1d8cf",
      "a360aebdc3b3419f877d0b3c7af743d2",
      "e8961c9555014ff68f2bb5c863631847",
      "210d2e3ff51241119eafebe69b540613",
      "8ea257dc83b949bfab5c5e0b10ecc80e",
      "0d7b3e87085b441bb1c3211994108848",
      "cae4ea0819d9445cba0cc3f86fb2bc12",
      "fb22f0d4629f462990e1fa8a336bf7d3",
      "82b63e884c6c40a2be7dd271d6224182",
      "496c4f6866484409b261ec63f23a4d86",
      "b6fac618904a4f1a9ef96aa3f07e9b43",
      "c1f64047b4bf44c9bb21e049bb93de2f",
      "0891dc3395e34675a832e29881385a8f",
      "d85edfe1e23a4ec1aedf383ebbd4a1f4",
      "e9708091e6004c10b44e27f4a3ddd08c",
      "7cbbbd8affca41c59cccbb5bcebf4bb9",
      "403506d2e0d84994b36f41fe507e96c2"
     ]
    },
    "id": "VLI6T_8ZVqMJ",
    "outputId": "19d9a1b5-3959-45c8-e145-98aa572daa34"
   },
   "outputs": [],
   "source": [
    "# HIDE OUTPUT\n",
    "# Choose your seeds to morph through and the number of steps to\n",
    "# take to get to each.\n",
    "\n",
    "SEEDS = [6624,6618,6616] # Better for faces\n",
    "#SEEDS = [1000,1003,1001] # Better for fish\n",
    "STEPS = 100\n",
    "\n",
    "# Remove any prior results\n",
    "!rm /content/results/*\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "os.makedirs(\"./results/\", exist_ok=True)\n",
    "\n",
    "# Generate the images for the video.\n",
    "idx = 0\n",
    "for i in range(len(SEEDS)-1):\n",
    "  v1 = seed2vec(G, SEEDS[i])\n",
    "  v2 = seed2vec(G, SEEDS[i+1])\n",
    "\n",
    "  diff = v2 - v1\n",
    "  step = diff / STEPS\n",
    "  current = v1.copy()\n",
    "\n",
    "  for j in tqdm(range(STEPS), desc=f\"Seed {SEEDS[i]}\"):\n",
    "    current = current + step\n",
    "    img = generate_image(device, G, current)\n",
    "    img.save(f'./results/frame-{idx}.png')\n",
    "    idx+=1\n",
    "\n",
    "# Link the images into a video.\n",
    "!ffmpeg -r 30 -i /content/results/frame-%d.png -vcodec mpeg4 -y movie.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKWZQwJP7KDu"
   },
   "source": [
    "You can now download the generated video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "iQ5POSA77PFA",
    "outputId": "c58b55ae-965e-4add-fda6-dafa5e4b18ac"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('movie.mp4')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
