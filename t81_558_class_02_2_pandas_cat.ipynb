{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_deep_learning/blob/main/t81_558_class_02_2_pandas_cat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "\n",
    "**Module 2: Python for Machine Learning**\n",
    "\n",
    "- Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "- For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 Material\n",
    "\n",
    "Main video lecture:\n",
    "\n",
    "- Part 2.1: Introduction to Pandas [[Video]](https://www.youtube.com/watch?v=wixHCvnvnsU&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_02_1_python_pandas.ipynb)\n",
    "- **Part 2.2: Categorical Values** [[Video]](https://www.youtube.com/watch?v=Fm7Ax23hDP0&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_02_2_pandas_cat.ipynb)\n",
    "- Part 2.3: Grouping, Sorting, and Shuffling in Python Pandas [[Video]](https://www.youtube.com/watch?v=tUhaD8xWd7k&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_02_3_pandas_grouping.ipynb)\n",
    "- Part 2.4: Using Apply and Map in Pandas [[Video]](https://www.youtube.com/watch?v=YNo_mg1RrkM&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_02_4_pandas_functional.ipynb)\n",
    "- Part 2.5: Feature Engineering in Pandas for Deep Learning in PyTorch [[Video]](https://www.youtube.com/watch?v=ezaVtM405Qs&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_02_5_pandas_features.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running the correct version of TensorFlow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2.2: Categorical and Continuous Values\n",
    "\n",
    "Neural networks require their input to be a fixed number of columns. This input format is very similar to spreadsheet data; it must be entirely numeric. It is essential to represent the data so that the neural network can train from it. Before we look at specific ways to preprocess data, it is important to consider four basic types of data, as defined by [[Cite:stevens1946theory]](http://psychology.okstate.edu/faculty/jgrice/psyc3214/Stevens_FourScales_1946.pdf). Statisticians commonly refer to as the [levels of measure](https://en.wikipedia.org/wiki/Level_of_measurement):\n",
    "\n",
    "- Character Data (strings)\n",
    "  - **Nominal** - Individual discrete items, no order. For example, color, zip code, and shape.\n",
    "  - **Ordinal** - Individual distinct items have an implied order. For example, grade level, job title, Starbucks(tm) coffee size (tall, vente, grande)\n",
    "- Numeric Data\n",
    "  - **Interval** - Numeric values, no defined start. For example, temperature. You would never say, \"yesterday was twice as hot as today.\"\n",
    "  - **Ratio** - Numeric values, clearly defined start. For example, speed. You could say, \"The first car is going twice as fast as the second.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Continuous Values\n",
    "\n",
    "One common transformation is to normalize the inputs. It is sometimes valuable to normalize numeric inputs in a standard form so that the program can easily compare these two values. Consider if a friend told you that he received a 10-dollar discount. Is this a good deal? Maybe. But the cost is not normalized. If your friend purchased a car, the discount is not that good. If your friend bought lunch, this is an excellent discount!\n",
    "\n",
    "Percentages are a prevalent form of normalization. If your friend tells you they got 10% off, we know that this is a better discount than 5%. It does not matter how much the purchase price was. One widespread machine learning normalization is the Z-Score:\n",
    "\n",
    "$$ z = \\frac{x - \\mu}{\\sigma} $$\n",
    "\n",
    "To calculate the Z-Score, you also need to calculate the mean(&mu; or $\\bar{x}$) and the standard deviation (&sigma;). You can calculate the mean with this equation:\n",
    "\n",
    "$$ \\mu = \\bar{x} = \\frac{x_1+x_2+\\cdots +x_n}{n} $$\n",
    "\n",
    "The standard deviation is calculated as follows:\n",
    "\n",
    "$$ \\sigma = \\sqrt{\\frac{1}{N} \\sum\\_{i=1}^N (x_i - \\mu)^2} $$\n",
    "\n",
    "The following Python code replaces the mpg with a z-score. Cars with average MPG will be near zero, above zero is above average, and below zero is below average. Z-Scores more that 3 above or below are very rare; these are outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\", na_values=[\"NA\", \"?\"]\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 7)\n",
    "pd.set_option(\"display.max_rows\", 5)\n",
    "\n",
    "df[\"mpg\"] = zscore(df[\"mpg\"])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Values as Dummies\n",
    "\n",
    "The traditional means of encoding categorical values is to make them dummy variables. This technique is also called one-hot-encoding. Consider the following data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values=[\"NA\", \"?\"],\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 7)\n",
    "pd.set_option(\"display.max_rows\", 5)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _area_ column is not numeric, so you must encode it with one-hot encoding. We display the number of areas and individual values. There are just four values in the _area_ categorical variable in this case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = list(df[\"area\"].unique())\n",
    "print(f\"Number of areas: {len(areas)}\")\n",
    "print(f\"Areas: {areas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four unique values in the _area_ column. To encode these dummy variables, we would use four columns, each representing one of the areas. For each row, one column would have a value of one, the rest zeros. For this reason, this type of encoding is sometimes called one-hot encoding. The following code shows how you might encode the values \"a\" through \"d.\" The value A becomes [1,0,0,0] and the value B becomes [0,1,0,0].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies([\"a\", \"b\", \"c\", \"d\"], prefix=\"area\",dtype=int)\n",
    "print(dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now encode the actual column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(df[\"area\"], prefix=\"area\",dtype=int)\n",
    "print(dummies[0:10])  # Just show the first 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the new dummy/one hot encoded values to be of any use, they must be merged back into the data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encode the _area_ column, we use the following code. Note that it is necessary to merge these dummies back into the data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 0)\n",
    "pd.set_option(\"display.max_rows\", 10)\n",
    "\n",
    "display(df[[\"id\", \"job\", \"area\", \"income\", \"area_a\", \"area_b\", \"area_c\", \"area_d\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, you will remove the original column _area_ because the goal is to get the data frame to be entirely numeric for the neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 0)\n",
    "pd.set_option(\"display.max_rows\", 5)\n",
    "\n",
    "df.drop(\"area\", axis=1, inplace=True)\n",
    "display(df[[\"id\", \"job\", \"income\", \"area_a\", \"area_b\", \"area_c\", \"area_d\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the First Level\n",
    "\n",
    "The **pd.concat** function also includes a parameter named _drop_first_, which specifies whether to get k-1 dummies out of k categorical levels by removing the first level. Why would you want to remove the first level, in this case, _area_a_? This technique provides a more efficient encoding by using the ordinarily unused encoding of [0,0,0]. We encode the _area_ to just three columns and map the categorical value of _a_ to [0,0,0]. The following code demonstrates this technique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies([\"a\", \"b\", \"c\", \"d\"], prefix=\"area\", drop_first=True, dtype=int)\n",
    "print(dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above data, the _area_a_ column is missing, as it **get_dummies** replaced it by the encoding of [0,0,0]. The following code shows how to apply this technique to a dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values=[\"NA\", \"?\"],\n",
    ")\n",
    "\n",
    "# encode the area column as dummy variables\n",
    "dummies = pd.get_dummies(df[\"area\"], drop_first=True, prefix=\"area\")\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "df.drop(\"area\", axis=1, inplace=True)\n",
    "\n",
    "# display the encoded dataframe\n",
    "pd.set_option(\"display.max_columns\", 0)\n",
    "pd.set_option(\"display.max_rows\", 10)\n",
    "\n",
    "display(df[[\"id\", \"job\", \"income\", \"area_b\", \"area_c\", \"area_d\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Encoding for Categoricals\n",
    "\n",
    "Target encoding is a popular technique for Kaggle competitions. Target encoding can sometimes increase the predictive power of a machine learning model. However, it also dramatically increases the risk of overfitting. Because of this risk, you must take care of using this method.\n",
    "\n",
    "Generally, target encoding can only be used on a categorical feature when the output of the machine learning model is numeric (regression).\n",
    "\n",
    "The concept of target encoding is straightforward. For each category, we calculate the average target value for that category. Then to encode, we substitute the percent corresponding to the category that the categorical value has. Unlike dummy variables, where you have a column for each category with target encoding, the program only needs a single column. In this way, target coding is more efficient than dummy variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a small sample dataset\n",
    "np.random.seed(43)\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"cont_9\": np.random.rand(10) * 100,\n",
    "        \"cat_0\": [\"dog\"] * 5 + [\"cat\"] * 5,\n",
    "        \"cat_1\": [\"wolf\"] * 9 + [\"tiger\"] * 1,\n",
    "        \"y\": [1, 0, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "    }\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 0)\n",
    "pd.set_option(\"display.max_rows\", 0)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to change them to a number rather than creating dummy variables for \"dog\" and \"cat,\" we would like to change them to a number. We could use 0 for a cat and 1 for a dog. However, we can encode more information than just that. The simple 0 or 1 would also only work for one animal. Consider what the mean target value is for cat and dog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means0 = df.groupby(\"cat_0\")[\"y\"].mean().to_dict()\n",
    "means0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The danger is that we are now using the target value ($y$) for training. This technique will potentially lead to overfitting. The possibility of overfitting is even greater if a small number of a particular category. To prevent this from happening, we use a weighting factor. The stronger the weight, the more categories with fewer values will tend towards the overall average of $y$. You can perform this calculation as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"y\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can implement target encoding as follows. For more information on Target Encoding, refer to the article [\"Target Encoding Done the Right Way\"](https://maxhalford.github.io/blog/target-encoding/), that I based this code upon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_smooth_mean(df1, df2, cat_name, target, weight):\n",
    "    # Compute the global mean\n",
    "    mean = df[target].mean()\n",
    "\n",
    "    # Compute the number of values and the mean of each group\n",
    "    agg = df.groupby(cat_name)[target].agg([\"count\", \"mean\"])\n",
    "    counts = agg[\"count\"]\n",
    "    means = agg[\"mean\"]\n",
    "\n",
    "    # Compute the \"smoothed\" means\n",
    "    smooth = (counts * means + weight * mean) / (counts + weight)\n",
    "\n",
    "    # Replace each value by the according smoothed mean\n",
    "    if df2 is None:\n",
    "        return df1[cat_name].map(smooth)\n",
    "    else:\n",
    "        return df1[cat_name].map(smooth), df2[cat_name].map(smooth.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code encodes these two categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT = 5\n",
    "df[\"cat_0_enc\"] = calc_smooth_mean(\n",
    "    df1=df, df2=None, cat_name=\"cat_0\", target=\"y\", weight=WEIGHT\n",
    ")\n",
    "df[\"cat_1_enc\"] = calc_smooth_mean(\n",
    "    df1=df, df2=None, cat_name=\"cat_1\", target=\"y\", weight=WEIGHT\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 0)\n",
    "pd.set_option(\"display.max_rows\", 0)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Values as Ordinal\n",
    "\n",
    "Typically categoricals will be encoded as dummy variables. However, there might be other techniques to convert categoricals to numeric. Any time there is an order to the categoricals, a number should be used. Consider if you had a categorical that described the current education level of an individual.\n",
    "\n",
    "- Kindergarten (0)\n",
    "- First Grade (1)\n",
    "- Second Grade (2)\n",
    "- Third Grade (3)\n",
    "- Fourth Grade (4)\n",
    "- Fifth Grade (5)\n",
    "- Sixth Grade (6)\n",
    "- Seventh Grade (7)\n",
    "- Eighth Grade (8)\n",
    "- High School Freshman (9)\n",
    "- High School Sophomore (10)\n",
    "- High School Junior (11)\n",
    "- High School Senior (12)\n",
    "- College Freshman (13)\n",
    "- College Sophomore (14)\n",
    "- College Junior (15)\n",
    "- College Senior (16)\n",
    "- Graduate Student (17)\n",
    "- PhD Candidate (18)\n",
    "- Doctorate (19)\n",
    "- Post Doctorate (20)\n",
    "\n",
    "The above list has 21 levels and would take 21 dummy variables to encode. However, simply encoding this to dummies would lose the order information. Perhaps the most straightforward approach would be to simply number them and assign the category a single number equal to the value in the parenthesis above. However, we might be able to do even better. A graduate student is likely more than a year so you might increase one value.\n",
    "\n",
    "## High Cardinality Categorical\n",
    "\n",
    "If there were many, perhaps thousands or tens of thousands, then one-hot encoding is no longer a good choice. We call these cases high cardinality categorical. We generally encode such values with an embedding layer, which we will discuss later when introducing natural language processing (NLP).\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.9 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
